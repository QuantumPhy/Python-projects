{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "LUT MODEL\n",
    "===========\n",
    "====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 64, 1)\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "embedding_size=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size,1), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] +1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(batches):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  batches=list(map(list, zip(*batches)))\n",
    "  s=list(\"\")\n",
    "  for b in batches:\n",
    "        ss=\"\"\n",
    "        for c in b:\n",
    "            ss+=id2char(int(c))\n",
    "        s.append(ss)\n",
    "  return s\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print((np.array(train_batches.next())).shape)\n",
    "print(characters(train_batches.next()))\n",
    "print(characters(train_batches.next()))\n",
    "print(characters(valid_batches.next()))\n",
    "print(characters(valid_batches.next()))\n",
    "\n",
    "# ==========================\n",
    "# OTHER EVALUATION FUNCTIONS\n",
    "# ==========================\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, embedding_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 64)\n",
      "(640, 10)\n",
      "(640, 10)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "vocabulary_size=27\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # first parameter of each gate in 1 matrix:\n",
    "  i_all = tf.Variable(tf.truncated_normal([1, 4*num_nodes], -0.1, 0.1))\n",
    "  # second parameter of each gate in 1 matrix\n",
    "  o_all = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases. \n",
    " \n",
    " \n",
    "  #embeddings\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], 0.0, 1.0))\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, 1],stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  b = tf.Variable(tf.zeros([1]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_mat=tf.matmul(i,i_all)\n",
    "    output_mat=tf.matmul(o,o_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(input_mat[:,0:num_nodes] + output_mat[:,0:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(input_mat[:,num_nodes:2*num_nodes] + output_mat[:,num_nodes:2*num_nodes] + fb)\n",
    "    update = input_mat[:,2*num_nodes:3*num_nodes] + output_mat[:,2*num_nodes:3*num_nodes] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(input_mat[:,3*num_nodes:] + output_mat[:,3*num_nodes:] + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,1]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    outputs_concat=tf.concat(outputs,0)\n",
    "    print((outputs_concat).shape)\n",
    "    #outputs_concat=tf.to_int32(outputs_concat)\n",
    "    logits=tf.nn.xw_plus_b(outputs_concat,w,b)\n",
    "    logits=tf.to_int32(logits)\n",
    "    embedded_logits = tf.nn.embedding_lookup(embeddings, logits[:,0])\n",
    "    embedded_logits=tf.to_float(embedded_logits)\n",
    "    print((embedded_logits).shape)\n",
    "    \n",
    "    train_labels=tf.to_int32(tf.concat(train_labels,0))\n",
    "    embedded_labels=tf.nn.embedding_lookup(embeddings,train_labels[:,0])\n",
    "    embedded_labels=tf.to_float(embedded_labels)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=embedded_labels, logits=embedded_logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(embedded_logits)\n",
    "  print(train_prediction.shape)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  \n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, 1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        \n",
    "        \n",
    "    sample_output=tf.concat(sample_output,0)\n",
    "    sample_logits=tf.nn.xw_plus_b(sample_output, w, b)\n",
    "    sample_logits=tf.to_int32(sample_logits)\n",
    "    print(sample_logits.shape)\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    embedded_sample_logits = tf.nn.embedding_lookup(normalized_embeddings, sample_logits[:,0])\n",
    "    embedded_sample_logits=tf.to_float(embedded_sample_logits)\n",
    "    \n",
    "    similarity = tf.matmul(embedded_sample_logits, tf.transpose(normalized_embeddings))\n",
    "    sample_prediction = tf.nn.softmax(embedded_sample_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 12.193330 learning rate: 10.000000\n",
      "640\n",
      "Minibatch perplexity:  197467.715335\n",
      "================================================================================\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "e                                                                               \n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "l                                                                               \n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "n                                                                               \n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "b                                                                               \n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "f                                                                               \n",
      "================================================================================\n",
      "Average loss at step 500: 21446228.666803 learning rate: 10.000000\n",
      "640\n",
      "Minibatch perplexity:  inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:25: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 1000: 148755848.640000 learning rate: 10.000000\n",
      "640\n",
      "Minibatch perplexity:  inf\n",
      "================================================================================\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "p                                                                               \n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "p                                                                               \n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "t                                                                               \n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "x                                                                               \n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "c                                                                               \n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-cc9c9b422dd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m       \u001b[1;31m#print((feed_dict[train_data[i]]).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedded_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 500\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "      #print((feed_dict[train_data[i]]).shape)\n",
    "    \n",
    "    _, l, predictions, lr,embedding,labels = session.run([optimizer, loss, train_prediction, learning_rate,embeddings,embedded_labels], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      \n",
    "      print(labels.shape[0])\n",
    "      print('Minibatch perplexity: ',np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      if step % (summary_frequency * 2) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = np.matrix([[np.random.randint(0,27)]])\n",
    "          #print(feed.shape)\n",
    "          sentence = id2char(feed[0][0])\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            feed = sample_logits.eval({sample_input: feed})\n",
    "            print(feed)\n",
    "            sentence += id2char(feed)\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      #for _ in range(valid_size):\n",
    "       # b = valid_batches.next()\n",
    "        #predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        #valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      #print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
