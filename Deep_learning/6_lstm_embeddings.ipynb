{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "LUT MODEL\n",
    "===========\n",
    "====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 64)\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "embedding_size=5\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] +1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(batches):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  batches=list(map(list, zip(*batches)))\n",
    "  s=list(\"\")\n",
    "  for b in batches:\n",
    "        ss=\"\"\n",
    "        for c in b:\n",
    "            ss+=id2char(int(c))\n",
    "        s.append(ss)\n",
    "  return s\n",
    "\n",
    "def characters2(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print((np.array(train_batches.next())).shape)\n",
    "print(characters(train_batches.next()))\n",
    "print(characters(train_batches.next()))\n",
    "print(characters(valid_batches.next()))\n",
    "print(characters(valid_batches.next()))\n",
    "\n",
    "# ==========================\n",
    "# OTHER EVALUATION FUNCTIONS\n",
    "# ==========================\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 26.0, size=[1, 1])\n",
    "  return b[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 27)\n",
      "(640, 27)\n",
      "(1, 27)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "vocabulary_size=27\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # first parameter of each gate in 1 matrix:\n",
    "  i_all = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1))\n",
    "  # second parameter of each gate in 1 matrix\n",
    "  o_all = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases. \n",
    " \n",
    " \n",
    "  #embeddings\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size],-0.1,0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_mat=tf.matmul(i,i_all)\n",
    "    output_mat=tf.matmul(o,o_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(input_mat[:,0:num_nodes] + output_mat[:,0:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(input_mat[:,num_nodes:2*num_nodes] + output_mat[:,num_nodes:2*num_nodes] + fb)\n",
    "    update = input_mat[:,2*num_nodes:3*num_nodes] + output_mat[:,2*num_nodes:3*num_nodes] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(input_mat[:,3*num_nodes:] + output_mat[:,3*num_nodes:] + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings+1):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    embedded_i=tf.nn.embedding_lookup(embeddings,i)\n",
    "    output, state = lstm_cell(embedded_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    outputs_concat=tf.concat(outputs,0)\n",
    "    #try to compute similarity here as well\n",
    "    logits=tf.nn.xw_plus_b(outputs_concat,w,b)\n",
    "    print((logits).shape)\n",
    "    \n",
    "    #Compute one hot encodings\n",
    "    label_batch=tf.concat(train_labels,0)\n",
    "    sparse_labels = tf.reshape(label_batch, [-1, 1])\n",
    "    derived_size = tf.shape(label_batch)[0]\n",
    "    indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n",
    "    concated = tf.concat([indices, sparse_labels],1)\n",
    "    outshape = tf.stack([derived_size, vocabulary_size])\n",
    "    labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 10000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  print(train_prediction.shape)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  #valid_dataset=np.array([i for i in range(27)])\n",
    "  #valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    " \n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  embedded_sample=tf.nn.embedding_lookup(embeddings,sample_input)\n",
    "  sample_output, sample_state = lstm_cell(embedded_sample, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        \n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    #similarity = tf.matmul(sample_prediction, tf.transpose(normalized_embeddings))\n",
    "    print(sample_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297918 learning rate: 10.000000\n",
      "================================================================================\n",
      "ptdoxrnd ee jcegptlaohrn npobe zfhymswhiorytnecwlmeftesvo otieeyadlbpfskxwxs f u\n",
      "oyapa  n uqmsf  rb  ys chzgnt elxr ysw odwoizvu onajtgisy joghxl zvoeonomnfizxc \n",
      "nee lichjykeiobotl xturorzsetiejodthfl ewsetxhuexxhubgtteie setdgtphrgtffjvrmxeg\n",
      "nsfidxsjoikooemx dn zhjsxrntd sett sg jda aneokb osfknanrercj ismnaljzmag sswiys\n",
      "gnslozcn f  vfwenkogzo uti robteidtrprogztesf lmacmtu evnxbiknrboaezkvhhe ku ptj\n",
      "================================================================================\n",
      "Average loss at step 500: 2.118223 learning rate: 10.000000\n",
      "Average loss at step 1000: 1.836499 learning rate: 10.000000\n",
      "================================================================================\n",
      "ning itial spicmal partylorily forols c offililie dimbhiward in wro storck is an\n",
      "ent by magers the that in his of he pneinjes a mina is six be maise colictionaic\n",
      "ber of any and iw from afow intarany and an and and an in the hind of ther hall \n",
      "ation anred butrety has crut enginationative in aretors ars as s tiference and s\n",
      "pulent send conte runir cerotions  galf to mrrison nine is works incluter mertio\n",
      "================================================================================\n",
      "Average loss at step 1500: 1.758802 learning rate: 10.000000\n",
      "Average loss at step 2000: 1.723900 learning rate: 10.000000\n",
      "================================================================================\n",
      "kol dnateer and the noweing liter spitely the ranby that compless in one in they\n",
      "y and partailine the few two s the structures and sceconted be o graped from the\n",
      "ain tuipor uniter the sceside of the four dewf this the desper unorshamed the pr\n",
      "putional of the greater and they and such langost texter was the is the bowo fir\n",
      "xsing officiaster is the hatto was the des to they the from stonfay mass though \n",
      "================================================================================\n",
      "Average loss at step 2500: 1.707048 learning rate: 10.000000\n",
      "Average loss at step 3000: 1.698530 learning rate: 10.000000\n",
      "================================================================================\n",
      "qunely one nine two there begourt with tran row libernetion formative canrey vic\n",
      "gar two e vg i and in pubir one nine three two to gund by and syry elliein propo\n",
      "ing tran toghtrap no hil one three raje acces of divide spanghames spards yance \n",
      "oun position elfructh the sainectly to ear a exettal and is one oil and oth bot \n",
      "x arack arthualy in bolr rathen unitm abrister thanketa britter in wordy and caz\n",
      "================================================================================\n",
      "Average loss at step 3500: 1.700174 learning rate: 10.000000\n",
      "Average loss at step 4000: 1.705371 learning rate: 10.000000\n",
      "================================================================================\n",
      "racy timphine to also the one five six shot and subling have interptester sour a\n",
      "polloc to deternng and soles goles of usual mest and falss abourtical it indian \n",
      "voistion rourth choses hi presivers of flated one orchict of husdi wils repre pr\n",
      "jal march ablick vire wife irlede that and other cartative of genenue ball been \n",
      "xicigus of biaz in exuction impe one zero eight the neetion at son howzersible c\n",
      "================================================================================\n",
      "Average loss at step 4500: 1.685396 learning rate: 10.000000\n",
      "Average loss at step 5000: 1.688929 learning rate: 10.000000\n",
      "================================================================================\n",
      "nder his death his jates is plrat birking clusly to the point zeul his a sather \n",
      "king on their the prophe it occepotulang but but fon commerninda ganbal on one n\n",
      "xhing from the is ninotion will leath focuink angahring refrismal sold a d or is\n",
      "onso for englisn ertian e is it sprefrications of is onsed almistre to like ican\n",
      "en thar one zero five the galasogy the techle roser by used by english it and bd\n",
      "================================================================================\n",
      "Average loss at step 5500: 1.665016 learning rate: 10.000000\n",
      "Average loss at step 6000: 1.656708 learning rate: 10.000000\n",
      "================================================================================\n",
      "pourd live arthor about a conld enverter all ganale hracker mother a hunded over\n",
      "st fhering the fooos baline in the dayers the ip that of helple on perlernard fo\n",
      "uce aru aruratect swert the later the one eighraid for the a spassite beelations\n",
      "ner of nemoble of get compaid two one seven to the place has two purent wough us\n",
      "ken ennaricast hebith a chard as be objection would mh to macaciam piritaki in t\n",
      "================================================================================\n",
      "Average loss at step 6500: 1.633899 learning rate: 10.000000\n",
      "Average loss at step 7000: 1.668925 learning rate: 10.000000\n",
      "================================================================================\n",
      "ing a becoun the maertues depig musteries in moressi divers racturefeigy they pr\n",
      "terod and this for mei simmonered thre sep contry capat be costist better arters\n",
      "ch is the deer bies to one nine a seven two ear of pas some imrafers exigun orig\n",
      "ences with area by artarated to one nine five come wishmbing essairs kt with goo\n",
      "jencture their enginging an port is pring of retorban erpannon finites feakt the\n",
      "================================================================================\n",
      "Average loss at step 7500: 1.661684 learning rate: 10.000000\n",
      "Average loss at step 8000: 1.661698 learning rate: 10.000000\n",
      "================================================================================\n",
      "witor singlander nury absurments if decaid two zero one his edamame rechurt lack\n",
      "by in a languaged recressurs haiving by set mues in conculd mound one ourchivali\n",
      "st d y gunnied zeonizzer retectery ancle meang pen one nine four in center rout \n",
      "ter in scienciage used at than flam americs d pairing storys recunsent or the co\n",
      "st rhougo a burks ecebung das evfer decose stidian ane althongon in strifing fro\n",
      "================================================================================\n",
      "Average loss at step 8500: 1.662074 learning rate: 10.000000\n",
      "Average loss at step 9000: 1.647665 learning rate: 10.000000\n",
      "================================================================================\n",
      "roriclist nuto oser production shmear many the mubt instands astla also out inte\n",
      "went and and doely on the lognay latic bengle includes aveinine inseque is leape\n",
      "chic of betwenty the south in indicitaine the ore see in this jonnizeved tody be\n",
      "oon is blacks filed auritely it is she it of losslice w theirs s any one nine ze\n",
      "kires of den ectussed focing to the stocn by oveiral aiost pas for fwoun despitu\n",
      "================================================================================\n",
      "Average loss at step 9500: 1.673197 learning rate: 10.000000\n",
      "Average loss at step 10000: 1.681549 learning rate: 1.000000\n",
      "================================================================================\n",
      "x onlingable irigam crest and person dotative copulig and appa allow tover origi\n",
      "ine in one one nine nine five mokester coomanshus payest werts which becuate and\n",
      "xing which of azn power a and plesions seev bransons martish be or almaged of va\n",
      "zeror of rutal of general nampress anth musion the tandes at the salime many of \n",
      "ness can askers andemers ofsel after partahian loss on the playing sugkrs is ing\n",
      "================================================================================\n",
      "Average loss at step 10500: 1.614830 learning rate: 1.000000\n",
      "Average loss at step 11000: 1.617766 learning rate: 1.000000\n",
      "================================================================================\n",
      "brifical fd greatlogical docurity and and perate islante to the sylled katteroas\n",
      "kly urine of freedy becime the imparticucon in the one six six five four that en\n",
      "n accorder under vewnivers is the was hillay stred another from dradding are sel\n",
      "part as a pairtes die for scorsa the to tave remixing pount of hames crites sout\n",
      "p turn defeirdia of is singed they calshan whose film ase to the player two coun\n",
      "================================================================================\n",
      "Average loss at step 11500: 1.595969 learning rate: 1.000000\n",
      "Average loss at step 12000: 1.597682 learning rate: 1.000000\n",
      "================================================================================\n",
      "mining held petson and opparce sarthend was for joh unforms universion worlder s\n",
      "s back galfien to prons t feym exulties the known of a mitnuates hars his listan\n",
      "n wand near the vid gunaachind are internet to a femewit see from form eight ril\n",
      "ons tange by relections seal blu camps trooking six some that of at confired res\n",
      "ung mams to the conawnorce of certates a hagte and right incolling that freat of\n",
      "================================================================================\n",
      "Average loss at step 12500: 1.591387 learning rate: 1.000000\n",
      "Average loss at step 13000: 1.582274 learning rate: 1.000000\n",
      "================================================================================\n",
      "ity d united uses reratectle alsolonlzed under yage anl to mruzens of the jament\n",
      "posam bovers eveny martorousadian old a might invesumel inzvered of the saind pl\n",
      " that urdis and of hife fentcres inveg her only two eight times of heat of there\n",
      "ayed death weable colaric nothy paenst in as towgor emberary event s loye are it\n",
      "rewing how his sogab joebly earthed as parespian crediral to atmay four thremen \n",
      "================================================================================\n",
      "Average loss at step 13500: 1.575300 learning rate: 1.000000\n",
      "Average loss at step 14000: 1.579415 learning rate: 1.000000\n",
      "================================================================================\n",
      "n aeal gile a smally monew contral his community a circle subsin spenuinally was\n",
      "warkay from irelonee with anghuland catroned ket agger who the lacary for him th\n",
      "rate indingland vail hyle intervs he rose by signi fined has exmaeogy have praya\n",
      "pher of these of cath two visive enstant notest caucer year the struple indiana \n",
      "er ible squrts gy in todays pulje to the keyler sepreths of the calle both to th\n",
      "================================================================================\n",
      "Average loss at step 14500: 1.574350 learning rate: 1.000000\n",
      "Average loss at step 15000: 1.579048 learning rate: 1.000000\n",
      "================================================================================\n",
      "iny accent whom absenk allic contropran by former gremainates statied sophistica\n",
      "fh opean was remodenst in s to miblern as whosectri met and powel was portings a\n",
      "tueroons toniessilo lossint philes g is the century one seven eight th popular p\n",
      "y cription was relationshan that the hrarn to rome use is his the comitming the \n",
      "qt independence the more to into through sugers an interviever by the geologatri\n",
      "================================================================================\n",
      "Average loss at step 15500: 1.562505 learning rate: 1.000000\n",
      "Average loss at step 16000: 1.585180 learning rate: 1.000000\n",
      "================================================================================\n",
      "kathors atticcoroters war the a aurts puspeable or presly m one gelesition shwar\n",
      "stax and akngvers refulity supportion ecails the have religious varist of create\n",
      "jer there are of the by this aultized is one nine one deal provent on the status\n",
      "men netween with cegtic one froh the two s han see univenended of a gada s moble\n",
      "quire proppedis the one eight one longe of bechueuard oir the fleutic both pseog\n",
      "================================================================================\n",
      "Average loss at step 16500: 1.587244 learning rate: 1.000000\n",
      "Average loss at step 17000: 1.584023 learning rate: 1.000000\n",
      "================================================================================\n",
      "vents not russolititartia he work beer and gaids camput notter malter monterated\n",
      "xtra is neven of the penin harrhor have this druinate set apper the whose write \n",
      "lath pgotent an into the bother an a could a creaded their that was arreman drum\n",
      "urds and ofchanticla and gotenal germany orbise the oldibra of rammongives princ\n",
      "nation indiator the miti and he economiles avortion jent franla claxs one nine f\n",
      "================================================================================\n",
      "Average loss at step 17500: 1.570789 learning rate: 1.000000\n",
      "Average loss at step 18000: 1.568331 learning rate: 1.000000\n",
      "================================================================================\n",
      "k over g denoeds afried to bommert in as it oftens be depitation result on this \n",
      "w was stand are currency a body grouply it three of the do before develve beolle\n",
      "werence window to four eight corner would theneror one the tiate the maulting th\n",
      "ment to chamic to namen in are they demoy tracial ling used a fast one nire thut\n",
      "riply of puttern of all leodenker of iffuip word ava with despanter celerative t\n",
      "================================================================================\n",
      "Average loss at step 18500: 1.567412 learning rate: 1.000000\n",
      "Average loss at step 19000: 1.569307 learning rate: 1.000000\n",
      "================================================================================\n",
      "zard as practice atmac the adenanason he eight nighternary beacher punew celes w\n",
      "wless one nine three one eight mand story of fizectrapi their and lande yard of \n",
      "ury new of multifimeniales some zelo is some a bowerted one eight terefive opact\n",
      "ol bose enaming used with one seven the marthokt is bot existiols to comba that \n",
      "n classitions influence conteganged a ug disciptation two of theme only times th\n",
      "================================================================================\n",
      "Average loss at step 19500: 1.588430 learning rate: 1.000000\n",
      "Average loss at step 20000: 1.564979 learning rate: 0.100000\n",
      "================================================================================\n",
      "mome internet a peach betwatic tolgars spect of pan surting the syipported on fo\n",
      "le one nine nine three five three one nine nine zero zero zero zero leapteds rep\n",
      "ron republic yocketic are the expanicd son to a gas kazilal in now function empi\n",
      "o on from s of the lade of a beneless mosetr one nine nine seven the average and\n",
      "uscliefed to was authorial from the carabic laved is the majory to thereal two f\n",
      "================================================================================\n",
      "Average loss at step 20500: 1.605060 learning rate: 0.100000\n",
      "Average loss at step 21000: 1.577566 learning rate: 0.100000\n",
      "================================================================================\n",
      "zer also in adverai discide up of countly udeval specily woy yaws quaginino year\n",
      "simores could light in two zero zero zero l do the uuming one nine nine zero a f\n",
      "ing two nieved to intern in courtment ney two cateationhed fact proved streast m\n",
      "ing theyal clairtown at the two zero s one saids the declack their maham from qu\n",
      "man of at yanges to algu coloblosifi bether pecale epen mantary have four four s\n",
      "================================================================================\n",
      "Average loss at step 21500: 1.572643 learning rate: 0.100000\n",
      "Average loss at step 22000: 1.573093 learning rate: 0.100000\n",
      "================================================================================\n",
      "dity bedition shices on other suppurt in every in a forcier the one turk kinda w\n",
      "meni for west a capant from the medicions of a filmained at valurents to held ha\n",
      "res or isnio clintobinal five datechies robole fm docy to reases pro intentate m\n",
      "ons unus gy believe i a as collo begies urtom coloplocism returners part through\n",
      "ws trade with be is sedutherantic pallosh empired to giesum yuss they write fary\n",
      "================================================================================\n",
      "Average loss at step 22500: 1.571700 learning rate: 0.100000\n",
      "Average loss at step 23000: 1.572052 learning rate: 0.100000\n",
      "================================================================================\n",
      "t egange follow experate tux  abaeldly charias citires from the vieinnife of the\n",
      "pucherfication cubson three same present vitton two zero zero zero zero sextems \n",
      "xly has literal such one nine seven this chusting certengaband and juka onivel o\n",
      "enter two the deally northish the one nine doto s arorges such states benerar m \n",
      "jer en one nine two four one of the hunded coldane are and is board unlimaries a\n",
      "================================================================================\n",
      "Average loss at step 23500: 1.587769 learning rate: 0.100000\n",
      "Average loss at step 24000: 1.585586 learning rate: 0.100000\n",
      "================================================================================\n",
      "ve typh youbmpigs and new g loided the beriod variienberly sefficulating loning \n",
      "ack to live death for it marraymortual through right athrology of pepts sponded \n",
      "x in the t into the hive as lunke partans would technical to engina indipief mar\n",
      "zam this not and the trafisem teese the traninnithing of as volice po to range g\n",
      "wan squnn with fine one nine three the chcrated b costation was was bauthminaxo \n",
      "================================================================================\n",
      "Average loss at step 24500: 1.573031 learning rate: 0.100000\n",
      "Average loss at step 25000: 1.567979 learning rate: 0.100000\n",
      "================================================================================\n",
      "xplange the dt discore or his fwe study inolut one nine seven two one eight zero\n",
      "ally of the after one is one five nine st millies bnetwapewower the alhangy brot\n",
      "dism eight zero zero zero zero one as jecielly knight six uliasly ford ledalf up\n",
      "nougally weba theore of the a g tebemprametha campables ina world consip musicia\n",
      "v sy imanize clingsia residenties out for strupts as mass two zero micrap situin\n",
      "================================================================================\n",
      "Average loss at step 25500: 1.564140 learning rate: 0.100000\n",
      "Average loss at step 26000: 1.574949 learning rate: 0.100000\n",
      "================================================================================\n",
      "ing anotherne partimate victly equlal the down supports new politations spucking\n",
      "thed in netted few schimbels laved one one descrizimes godeling with mag to teru\n",
      " forpiecoming spherouge noted b una bal lintalias firier sualewnikh othish polyn\n",
      "boided the nange high s a marries placed to poriters value r often hill other re\n",
      "x nont cliged buck enomengred party and the change consonable molg six four zero\n",
      "================================================================================\n",
      "Average loss at step 26500: 1.564123 learning rate: 0.100000\n",
      "Average loss at step 27000: 1.563132 learning rate: 0.100000\n",
      "================================================================================\n",
      "xually oppose upator the wilol to one three five four zero vouqea that raulthest\n",
      "part suffailig manse or this is the feirer sowned three one internation to band \n",
      "kd one nine two five three and famiiet heathols i entituents of order or the nom\n",
      "un lineitural singors the proniness and large from undernabys national particula\n",
      "zer for athances has modern desaction officeas states has lele one nine four nin\n",
      "================================================================================\n",
      "Average loss at step 27500: 1.584733 learning rate: 0.100000\n",
      "Average loss at step 28000: 1.591392 learning rate: 0.100000\n",
      "================================================================================\n",
      "jents of at well an ome in jo bud during were segents of for time of early by bo\n",
      "er of dispaced but a banal require argument the art convell as and states samon \n",
      "men first to minively asdading prafies bnanains of eight four eight self arm out\n",
      "ur while of his rom and and the politate the non and indep munes a ierure one ei\n",
      "va micechus bronged cire is g within toward and similar kingy begin bidied inclu\n",
      "================================================================================\n",
      "Average loss at step 28500: 1.575771 learning rate: 0.100000\n",
      "Average loss at step 29000: 1.574683 learning rate: 0.100000\n",
      "================================================================================\n",
      "kered smith home where yistoselingly suld live jyter occide one a cautority were\n",
      "jent patent six one six seven three four nine wible conqmoweption carbow muk to \n",
      "ver to magrobbab alfologon we live to his ackould coment ronding this worked d t\n",
      "sehy and not of the lossan are i only mid a from have knewal jecplis readers as \n",
      "le four six suppertup to be state threat for the southern of the indemine machin\n",
      "================================================================================\n",
      "Average loss at step 29500: 1.568105 learning rate: 0.100000\n",
      "Average loss at step 30000: 1.585582 learning rate: 0.010000\n",
      "================================================================================\n",
      "hody serom peciwidel averous accept entered danges indixiscidg thenee which and \n",
      "cignfedtern of hemos city city two all for new invoedessial america the state co\n",
      "st must can euspres can interkament goin of attexts whind port rainhing ors iesm\n",
      "x with the exalitorvany chanames chirenks of jicpility mluside unnections tandas\n",
      "ver noxida their on littlerwi refisia s name fixhn unempors and the one nine fou\n",
      "================================================================================\n",
      "Average loss at step 30500: 1.571075 learning rate: 0.010000\n",
      "Average loss at step 31000: 1.558003 learning rate: 0.010000\n",
      "================================================================================\n",
      "urple which largest computed by memory to mrors passical on char bran isuas the \n",
      "ing that carybish meming projecys ball sumsaly found which some formed from in c\n",
      "neria cabining as traficial borqually buss are acsiesses civistration recort for\n",
      "zevalnozed in one one north stop are sep tature anyu there interring traxwillle \n",
      "dian laid repunaling are proposes ufsky prepomated at one nine one eight zero th\n",
      "================================================================================\n",
      "Average loss at step 31500: 1.574116 learning rate: 0.010000\n",
      "Average loss at step 32000: 1.568405 learning rate: 0.010000\n",
      "================================================================================\n",
      "jen governint cannato pulet get depiantic and personed to are for the empire for\n",
      "dating curresm of the astouts the serge and adad to the making mitted they one n\n",
      "lise varesia in the cociation zy eight one samefards eight seven once edrimoums \n",
      "jen shap sultugans then she hrogtration and natio by a diath constitured hysa un\n",
      "n for and remained however the construseccpiling inamorduned bointy and premidos\n",
      "================================================================================\n",
      "Average loss at step 32500: 1.572053 learning rate: 0.010000\n",
      "Average loss at step 33000: 1.574385 learning rate: 0.010000\n",
      "================================================================================\n",
      "zal were the north in dude dieced one eight four part game presidenting can the \n",
      "ensit two two two such rololisit succemp to parrizany americandtage eooss succes\n",
      "inibres festem operation by him were out an empire of fousa spushums in the comp\n",
      "menivally the intersic and toway seroonan biving augurating oyuly properted ogav\n",
      "p to settlex user s one nine nine two one seven zero a s bemse and erdine tom or\n",
      "================================================================================\n",
      "Average loss at step 33500: 1.576571 learning rate: 0.010000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-025ec8c552fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m       \u001b[1;31m#print((feed_dict[train_data[i]]).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_lab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 50001\n",
    "summary_frequency = 500\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "      #print((feed_dict[train_data[i]]).shape)\n",
    "    \n",
    "    _, l, predictions, lr,train_lab = session.run([optimizer, loss, train_prediction, learning_rate,train_labels], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      \n",
    "      #print(train_lab)\n",
    "      #print(labels.shape[0])\n",
    "      #print('Minibatch perplexity: ',np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      if step % (summary_frequency * 2) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "          feed[0,] =np.random.randint(0,27)\n",
    "          sentence = id2char(feed[0])\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction=sample_prediction.eval({sample_input:feed})\n",
    "            k = sample(prediction)\n",
    "            k=characters2(k)[0]\n",
    "            feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "            feed[0,] = char2id(k)\n",
    "            sentence += k\n",
    "            #feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "            #feed[0,] = np.argmax(prediction)\n",
    "            #print(feed.shape)\n",
    "            #sentence += id2char(feed[0,])\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      #for _ in range(valid_size):\n",
    "       # b = valid_batches.next()\n",
    "        #predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        #valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      #print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
