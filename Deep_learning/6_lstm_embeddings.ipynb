{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "LUT MODEL\n",
    "===========\n",
    "====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 64)\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "embedding_size=5\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = char2id(self._text[self._cursor[b]])\n",
    "      self._cursor[b] = (self._cursor[b] +1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(batches):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  batches=list(map(list, zip(*batches)))\n",
    "  s=list(\"\")\n",
    "  for b in batches:\n",
    "        ss=\"\"\n",
    "        for c in b:\n",
    "            ss+=id2char(int(c))\n",
    "        s.append(ss)\n",
    "  return s\n",
    "\n",
    "def characters2(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print((np.array(train_batches.next())).shape)\n",
    "print(characters(train_batches.next()))\n",
    "print(characters(train_batches.next()))\n",
    "print(characters(valid_batches.next()))\n",
    "print(characters(valid_batches.next()))\n",
    "\n",
    "# ==========================\n",
    "# OTHER EVALUATION FUNCTIONS\n",
    "# ==========================\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 26.0, size=[1, 1])\n",
    "  return b[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 27)\n",
      "(?, 1) indices.shape\n",
      "(640, 27)\n",
      "(1, 27)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "vocabulary_size=27\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # first parameter of each gate in 1 matrix:\n",
    "  i_all = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1))\n",
    "  # second parameter of each gate in 1 matrix\n",
    "  o_all = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases. \n",
    " \n",
    " \n",
    "  #embeddings\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size],-0.1,0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_mat=tf.matmul(i,i_all)\n",
    "    output_mat=tf.matmul(o,o_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(input_mat[:,0:num_nodes] + output_mat[:,0:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(input_mat[:,num_nodes:2*num_nodes] + output_mat[:,num_nodes:2*num_nodes] + fb)\n",
    "    update = input_mat[:,2*num_nodes:3*num_nodes] + output_mat[:,2*num_nodes:3*num_nodes] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(input_mat[:,3*num_nodes:] + output_mat[:,3*num_nodes:] + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings+1):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    embedded_i=tf.nn.embedding_lookup(embeddings,i)\n",
    "    output, state = lstm_cell(embedded_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    outputs_concat=tf.concat(outputs,0)\n",
    "    #try to compute similarity here as well\n",
    "    logits=tf.nn.xw_plus_b(outputs_concat,w,b)\n",
    "    print((logits).shape)\n",
    "    \n",
    "    #Compute one hot encodings\n",
    "    label_batch=tf.concat(train_labels,0)\n",
    "    sparse_labels = tf.reshape(label_batch, [-1, 1])\n",
    "    derived_size = tf.shape(label_batch)[0]\n",
    "    indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n",
    "    print(indices.shape,'indices.shape')\n",
    "    concated = tf.concat([indices, sparse_labels],1)\n",
    "    outshape = tf.stack([derived_size, vocabulary_size])\n",
    "    labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 10000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  print(train_prediction.shape)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  #valid_dataset=np.array([i for i in range(27)])\n",
    "  #valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    " \n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  embedded_sample=tf.nn.embedding_lookup(embeddings,sample_input)\n",
    "  sample_output, sample_state = lstm_cell(embedded_sample, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        \n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    #similarity = tf.matmul(sample_prediction, tf.transpose(normalized_embeddings))\n",
    "    print(sample_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299174 learning rate: 10.000000\n",
      "================================================================================\n",
      "z kejxiwxjeosnt tprfgwneigow f ffulngeqony kpatvq jbitiyaigni rzmpgmmslauehfuav \n",
      "fuyf  qndoyakllynyjzo cpsvt xr  ekvbijtwwudtnnfuktb lhmwt ywneedzzeeh gh tqi mo \n",
      "xj zstr n nisygoaiurnovbflu tixcwx eemi hyyvhi c hexngdefel spkqnpnfllbyvqlbumf \n",
      "hwonnfsqv t jd mn  iocjnsp z qaa  onnzxa stwt ismxeptdony eiaejisewasc wpztqphkk\n",
      "ltpuke dftleoet rs fd vdestezizcsjslnnlewboxbmnrvmamtpdwbpexlreos cetfxd xaifmio\n",
      "================================================================================\n",
      "Average loss at step 500: 2.145794 learning rate: 10.000000\n",
      "Average loss at step 1000: 1.839583 learning rate: 10.000000\n",
      "================================================================================\n",
      "niment by of sturs felketh and he direfter americtic in air as fol clas have sha\n",
      "y wills in in aro and in tich fivlion a letar wele time ennocient in a cincild a\n",
      "al wile as wan of siggent dilifionshosits he rome mmig acthially meaints the lec\n",
      "unisules anothin of the sifiler adbolarious h appar will wisan say bp not fofrok\n",
      "britary signeriaf popularary lisfitel revul danism inftelogohisate civlicual and\n",
      "================================================================================\n",
      "Average loss at step 1500: 1.761449 learning rate: 10.000000\n",
      "Average loss at step 2000: 1.723277 learning rate: 10.000000\n",
      "================================================================================\n",
      "mall poetrion where wwass mide of the thebyes their reputalbot put priculces maj\n",
      "incals med the succreame the sequen kidise truc emorom outs of imlise a janick e\n",
      "ved the cons bolputed stade his one nine nine imxize this and have is as in the \n",
      "ing grand and speces enstum scwere one nine seven zeron the one phpe the southem\n",
      "vers the from the bunc the the totner the expnense one nine eight zero four thre\n",
      "================================================================================\n",
      "Average loss at step 2500: 1.711644 learning rate: 10.000000\n",
      "Average loss at step 3000: 1.699343 learning rate: 10.000000\n",
      "================================================================================\n",
      "dion shation hincotanto one exchildislated and open thore rated time former bubl\n",
      "borstials to has dass jietropensit takarchon handago criation of lecteries in ca\n",
      "ved fownerf filsons the sinte game abled a all poumk colscantoards states in rie\n",
      "ently da zero zero un ciistificantentizcts one lurehts fle qurine than gather ci\n",
      "vils absix three five brigion use the calledi near thagh and iffcheedediunces s \n",
      "================================================================================\n",
      "Average loss at step 3500: 1.701381 learning rate: 10.000000\n",
      "Average loss at step 4000: 1.708963 learning rate: 10.000000\n",
      "================================================================================\n",
      "hn a and can sumamon hale by one nine tlast of and rower a estructs matabiatity \n",
      "me two gen it iss roh joinshimia it words soyk to untroned besiods in practions \n",
      "busl unnarticlly he minud bi one piot beagrabbally games figutist lodocic war va\n",
      "wegpromergate only s not nine four zero zero zero the cbardes the and ecorty are\n",
      "e to fram demoyavies more fedw shoogle rejendaons gious susony integrarial actor\n",
      "================================================================================\n",
      "Average loss at step 4500: 1.682434 learning rate: 10.000000\n",
      "Average loss at step 5000: 1.689021 learning rate: 10.000000\n",
      "================================================================================\n",
      "ur of mard voch on for one refers the is oh bipary in the thoule inum dayservanc\n",
      "viring the over cotes it deffacure caxe somits bunther one nine seven one one fo\n",
      "qer of marghting only palet thing qroginowleentazing relifted to do be rinconico\n",
      "ward of the mittion of milm whis fismies both the areum bonned accortives one of\n",
      "th prictraince of objys in referen nine pan anticus dischill a factwila a mark i\n",
      "================================================================================\n",
      "Average loss at step 5500: 1.665390 learning rate: 10.000000\n",
      "Average loss at step 6000: 1.657086 learning rate: 10.000000\n",
      "================================================================================\n",
      "s finak seast dnawevery is relatary were has eash aroundarces and the finst and \n",
      "can wests see hubsten desale presents eniia soilare olk the grreea when obth to \n",
      "heral the laws seriver begin weanas in fom zero the places mader main catebatter\n",
      "withale and stows zero kiach orige actare renamaic a norach weres and eversely o\n",
      "ood chaneed behase pnoch of extersheters craved can rolr cit flated he hiisility\n",
      "================================================================================\n",
      "Average loss at step 6500: 1.634969 learning rate: 10.000000\n",
      "Average loss at step 7000: 1.670364 learning rate: 10.000000\n",
      "================================================================================\n",
      "tister lesindsneus out twelea he lisple to been with the coummusain while the wi\n",
      "witch underra singd the and meatropes in seteg concurs or teur edact than the de\n",
      "orizate me intran memor the called of a movemed winose long saber of ju nuarish \n",
      "vers it sper had betbize tear world monordas wistif even find are cloance was re\n",
      "arcy and moth the un essamity the year of this peliter the commoneri are mleasm \n",
      "================================================================================\n",
      "Average loss at step 7500: 1.661764 learning rate: 10.000000\n",
      "Average loss at step 8000: 1.661760 learning rate: 10.000000\n",
      "================================================================================\n",
      "fin flawtory and former the oppistic of the using finar the lights these in plic\n",
      "y e pidh pake diggure plich exchist hypers s wry crudteated a tep grizist scamar\n",
      "fer i flimeidn due outslumes scherative univelst kmbisactel he relitian there st\n",
      " daster systemder ftugh were accoundress stater teh one seven nine eight and sta\n",
      "lignar farmed the statisin wilel few and county in the conditionnist dughmad few\n",
      "================================================================================\n",
      "Average loss at step 8500: 1.664483 learning rate: 10.000000\n",
      "Average loss at step 9000: 1.649592 learning rate: 10.000000\n",
      "================================================================================\n",
      "gent of ited and far malicant yuester is their unstandarticoms to ello s three n\n",
      "ing headt k and sachetarize in and herasi the eventudent in the bebational shrea\n",
      "hifican insentine up wests to a gregist in the indively if for that veroo of sit\n",
      "ensive of althoughal levite to stal a was irelesters interranced visue expect ei\n",
      "x four six five basiff placed and thetia and with and auchory places in the not \n",
      "================================================================================\n",
      "Average loss at step 9500: 1.674208 learning rate: 10.000000\n",
      "Average loss at step 10000: 1.681788 learning rate: 1.000000\n",
      "================================================================================\n",
      "cornapan a one of ocku russiunes epressors syriter maincamphor seven cordels pas\n",
      "y airs jan that the statal mayer links like char there sy also plansain of the e\n",
      "on by heream and part hermed for dammances ligfies is a solstan brite during one\n",
      "l in curver maden univers alshing other sites mystemver and the inforter for a v\n",
      "derse and one zero zero one twasting otherning pilitzed was schellates one one s\n",
      "================================================================================\n",
      "Average loss at step 10500: 1.613660 learning rate: 1.000000\n",
      "Average loss at step 11000: 1.619825 learning rate: 1.000000\n",
      "================================================================================\n",
      "lent medering in the sholide is inum off in on the prime the byzonom been includ\n",
      " if the cat assa scripressers as the combration preans group of the responsiloti\n",
      "ellisions annel depite value evenue acidinistrics need they for the lausely by t\n",
      "ing by bepon unqersa couster spepen for auttled two six being or grouphs ady and\n",
      "mentalt nor in the as krisprosyly and wasopjes is salonstand reportury a time la\n",
      "================================================================================\n",
      "Average loss at step 11500: 1.598109 learning rate: 1.000000\n",
      "Average loss at step 12000: 1.602219 learning rate: 1.000000\n",
      "================================================================================\n",
      "ntoria n dhs the cats often difpegeal some largon is includes that is ordament s\n",
      "x atwem kensinal strer of of sitalists at prize and his dr on north shuth is bec\n",
      "s textern khow plaonusal usly granjority and dynts as the up at the emperors to \n",
      "lon tower up in three six two cus that this son the had wests five d generations\n",
      "ation of the mervents onvicul s other compary stecepteon other these people whin\n",
      "================================================================================\n",
      "Average loss at step 12500: 1.593839 learning rate: 1.000000\n",
      "Average loss at step 13000: 1.585217 learning rate: 1.000000\n",
      "================================================================================\n",
      "oning with elrsule four imeter bazrac natize for the unsives van dui uncarnowade\n",
      "use agerean dokm anat go consources and has nhmstrarionoh and tonsture polizing \n",
      " schuabount focch to one five three sosge three seven five five imes divers peop\n",
      "hnited bruiding the nown such thiss beles four gird machy a duviblated the one t\n",
      "x gowlancheires this biis gaas kno that puasically appliated on these idegiat ex\n",
      "================================================================================\n",
      "Average loss at step 13500: 1.580290 learning rate: 1.000000\n",
      "Average loss at step 14000: 1.584632 learning rate: 1.000000\n",
      "================================================================================\n",
      " forkally or in inited ziscia kernature assigian was recires color to caled but \n",
      "hera of out theil nehp is deshical channel e in raik it final the caving the gin\n",
      "k on as all sixnian brookin absount a sovicts termeting of city dons is most sou\n",
      "k envided strets from iolation was subplainst make and postralicing considers li\n",
      "x one nine nine two a finx top two any the holring set but solous is for islade \n",
      "================================================================================\n",
      "Average loss at step 14500: 1.580410 learning rate: 1.000000\n",
      "Average loss at step 15000: 1.581710 learning rate: 1.000000\n",
      "================================================================================\n",
      "dabation than of popular of lot led goods of a certagil ocase from if calleds sa\n",
      "nance often ecost the defined the and firm on advant resingly boy him name asser\n",
      "x monelinn assa is a sainpalloncely taulors ii a compedrons of reput out of the \n",
      "less west of a what part officely turgis is the words of disfubstand extensed st\n",
      "x feromburs lelacs as de otherfust of the not hamas produtiods ununivest moslate\n",
      "================================================================================\n",
      "Average loss at step 15500: 1.561622 learning rate: 1.000000\n",
      "Average loss at step 16000: 1.584397 learning rate: 1.000000\n",
      "================================================================================\n",
      "res indive fleging ceast any this no machinomoret fish cardimage also ipi the fe\n",
      "ent cophept mess although bloreblyt mosts bandmativi kuness are the curyagi prob\n",
      "qed s standom ands identebly dia s lasyrranguage see include recognisus locating\n",
      "breper indemences dire area gst or get the dispactively are catebeat on the futu\n",
      "ver the wroadely the million at the due tribles things of taker summury anaye sh\n",
      "================================================================================\n",
      "Average loss at step 16500: 1.587833 learning rate: 1.000000\n",
      "Average loss at step 17000: 1.587503 learning rate: 1.000000\n",
      "================================================================================\n",
      "x on the pxace afternite of outh when a labx to because domin pariforme wormed f\n",
      "furey to be stuaging british well through formelvers office baked the composers \n",
      "re the erginal game astries to syning in arginia northeseingd waborg the astraft\n",
      "tholy governicals one word for synological estably and allow records whas seven \n",
      "guires in a seatures godning singe for pustnence on out abaghtom the in main see\n",
      "================================================================================\n",
      "Average loss at step 17500: 1.573217 learning rate: 1.000000\n",
      "Average loss at step 18000: 1.570719 learning rate: 1.000000\n",
      "================================================================================\n",
      "s from main skaricd in one nine five one seven hearcy as for lise of he the hook\n",
      " and a non cornampical compleches whiser wepal produce united the prosous monad \n",
      "ventions bolity a first wille zore calling torgining dans y di jstacsined suppor\n",
      "ge k lost born native worlt often every insect with the law experencession with \n",
      "a the yeasphakonidails esmused while in the complencosition of finus to the and \n",
      "================================================================================\n",
      "Average loss at step 18500: 1.568334 learning rate: 1.000000\n",
      "Average loss at step 19000: 1.571729 learning rate: 1.000000\n",
      "================================================================================\n",
      "fruly commusity africa holts fdot offerset vitions refers to theres four five on\n",
      "s that between the finus is mo then the inteneer of theatracty the israel way to\n",
      "ment only games to chaters texplumnial decon in leanning into its bastly wextest\n",
      "s mags and elumatos preyed year in they alterned of the several a malazy paring \n",
      "formdhem of ritceetly depoywee the is frassional god two two herp then the usa a\n",
      "================================================================================\n",
      "Average loss at step 19500: 1.589651 learning rate: 1.000000\n",
      "Average loss at step 20000: 1.567431 learning rate: 0.100000\n",
      "================================================================================\n",
      "ments secternicles in one nine three in immercy of the assermets two governments\n",
      "lither agrice have at ckve disker is qark filmd a elemine long their charago mat\n",
      "jidate of particular att that viriauds is three one one seven one eight one zero\n",
      "lest the local of rivor the trades musinol falth mugre of the nebren succeetia a\n",
      "watentioched geyning as as the furthence the m solvance that silen over sink ger\n",
      "================================================================================\n",
      "Average loss at step 20500: 1.609716 learning rate: 0.100000\n",
      "Average loss at step 21000: 1.579646 learning rate: 0.100000\n",
      "================================================================================\n",
      "uns helished to a to been shortle inver to to three six when the stheticlle stud\n",
      "ust house often on janunijiania of the rochation for other contempolizate sprigy\n",
      "qer of is the imessian is a resprucled in one nine fives ynown two zero two one \n",
      "una eltacht peperand left in these at his an kan american catatch cathern and mo\n",
      "n a canomety of the plays the sometric keriun influence dna westined maturation \n",
      "================================================================================\n",
      "Average loss at step 21500: 1.573186 learning rate: 0.100000\n",
      "Average loss at step 22000: 1.577080 learning rate: 0.100000\n",
      "================================================================================\n",
      "ia the has aciarly the combates their by give north from players over were and a\n",
      "oria relberge in it deryned also uanced is no yweniples the searly belarer of a \n",
      "bid morist the theory do strence wimmer of biolum contragus phaces swege of grou\n",
      "k struct calmit growen in name that tax secital six seven ones zero five sometim\n",
      "witled on whiala signaton statish tablan lincee of a nemelties has column howeve\n",
      "================================================================================\n",
      "Average loss at step 22500: 1.576918 learning rate: 0.100000\n",
      "Average loss at step 23000: 1.575550 learning rate: 0.100000\n",
      "================================================================================\n",
      "zistor ere one nine two zero zero nine one zero zero zero zero zero zero zero fo\n",
      "wita of its or concention story the pebries called mels any guid light refers in\n",
      "trolic would currentnation the le including around with lawtic or in one nine ei\n",
      "k logogoty qeicas garted in boy in the elinf it laots disky crima aso warment up\n",
      "card in north a syppering any plastotian of visia painark squedimiss situmation \n",
      "================================================================================\n",
      "Average loss at step 23500: 1.589974 learning rate: 0.100000\n",
      "Average loss at step 24000: 1.588420 learning rate: 0.100000\n",
      "================================================================================\n",
      "y qued jubi articistition altiades of siiria or since ellurns of the life was th\n",
      "cial in bona two eqoding and where western exided be using develow sleading men \n",
      "a young in allown qeloboriable internatesmytame motolairence row with cable here\n",
      "ate most father is several and offlial one nine terrangers alegies is e sirale w\n",
      "tar article vioula them is a discoveros electry on the ritions a volex scot of d\n",
      "================================================================================\n",
      "Average loss at step 24500: 1.575857 learning rate: 0.100000\n",
      "Average loss at step 25000: 1.566750 learning rate: 0.100000\n",
      "================================================================================\n",
      "inced kil decomples came year did muctiod and electronor ships right biona use m\n",
      "p on sup become min envirue foodbable the clinkog to amighchabreasing betware ca\n",
      "qerings instorage the virge the relioung silud less bones thray bon them surfast\n",
      "thdegher undeep ands small x ffwurn are some monjomoyaded in image bis same and \n",
      "vestramed of dathed the english spret who central and begins melanting the gitin\n",
      "================================================================================\n",
      "Average loss at step 25500: 1.563473 learning rate: 0.100000\n",
      "Average loss at step 26000: 1.575364 learning rate: 0.100000\n",
      "================================================================================\n",
      "ex best uround work poll electronol giv like the albuted an incanariart lilion g\n",
      "y hardd after related these and pscwidion would and cospandable biggg tie the wi\n",
      "pe entracte the given persion prographicisting how offeria among three in lack a\n",
      "qens is greathor cold in the states united is the with city azom is nosing for t\n",
      " fujthes it soft mamila democations thoses azedi the either the are can d one me\n",
      "================================================================================\n",
      "Average loss at step 26500: 1.563511 learning rate: 0.100000\n",
      "Average loss at step 27000: 1.564592 learning rate: 0.100000\n",
      "================================================================================\n",
      "le of grainstrands preylision kird mugd with way chopitant sould dominelies was \n",
      "hn had electrice acrets which plates the he dodmustions of concentler decessiud \n",
      "us of stritic mean such essur blot one nine five main wanth golic forci ontist h\n",
      "ward was alto imaics varres shads in banomut of the presence a to good other is \n",
      "ple it is vi carden refumegion and have been of gay knegation one nine three zer\n",
      "================================================================================\n",
      "Average loss at step 27500: 1.586703 learning rate: 0.100000\n",
      "Average loss at step 28000: 1.597045 learning rate: 0.100000\n",
      "================================================================================\n",
      "pen suppress requrts mopte to mavility see nine six nine six one officials b had\n",
      "ifiin kakh exustwaristry civylest wonsing four one two necese two five one infla\n",
      "nuse either in legers who known of the winning internaments that standes and veg\n",
      "furn reams  the field quiul de event colen to the engw is nine using s gdow corv\n",
      "s of the one nine four was union of the leavoral the sailions scy based in da ge\n",
      "================================================================================\n",
      "Average loss at step 28500: 1.577821 learning rate: 0.100000\n",
      "Average loss at step 29000: 1.574616 learning rate: 0.100000\n",
      "================================================================================\n",
      "s iv the websels aleigo based the withh partants universia s el with the menty s\n",
      "ton in jate to juthers sy the understynd of on the no in juad the aliming direct\n",
      "les wind history newan writtuloo wabor the works or f missibit considered that t\n",
      "jowine k sourc or herap accaplors and is bably are stribut a between from report\n",
      "ned invers and excusmuhing three five five instras table of the reventeram from \n",
      "================================================================================\n",
      "Average loss at step 29500: 1.571277 learning rate: 0.100000\n",
      "Average loss at step 30000: 1.581754 learning rate: 0.010000\n",
      "================================================================================\n",
      "q extegni and can on assiftin the juado for phlacome no and rided a predico legi\n",
      "s syns star confists is sention ban linch and the chelve light computer one eigh\n",
      "quant agdands senning a swutte o and founding performillun shoveed for an prodic\n",
      "weries celruc the provide in these skmack s one two mikists the degal was a jind\n",
      "dwri house using numberseral in two zero zero m seviden zero bit be appeastative\n",
      "================================================================================\n",
      "Average loss at step 30500: 1.572484 learning rate: 0.010000\n",
      "Average loss at step 31000: 1.565869 learning rate: 0.010000\n",
      "================================================================================\n",
      "qerental is searle play kingdom ad derive which abount a diving ollar the counte\n",
      "x startadali of that astreat citary of the lepi decausent generate geargs of the\n",
      "k form impodech warmed for lelowtions l on he game starnatord a militables long \n",
      "d boon power after transrace was intellendibres operating compariled the t see a\n",
      "rioni al gree two six for jollars ministed when jangard the all liolus thane the\n",
      "================================================================================\n",
      "Average loss at step 31500: 1.578223 learning rate: 0.010000\n",
      "Average loss at step 32000: 1.569709 learning rate: 0.010000\n",
      "================================================================================\n",
      "us sbp requstions to a only one nine ricle from gambenchized fethibule featuring\n",
      "ptrayed europeaziss be several sollon was were special and frognima by electrian\n",
      "y issosle three monomyn psets procrestre hought on river joke of the released ma\n",
      "ssophal numbing ory pesersises warsed of the krisfors striag whisg several way i\n",
      "qets by aircasop s churchoings literous secondine tuincial dnits movically these\n",
      "================================================================================\n",
      "Average loss at step 32500: 1.573581 learning rate: 0.010000\n",
      "Average loss at step 33000: 1.574408 learning rate: 0.010000\n",
      "================================================================================\n",
      "jian up atal prigy that of redused to the jameanting this one bix activity press\n",
      "rain naws partund chemeting as three enfuld that the rist world are rosopheinges\n",
      "zew outlinets wales matthands as the sele prominet ashiles forcement lobing the \n",
      "wer untrovi are which edgucy computtirnical rung as parown one four eight mpt la\n",
      "and the regard aresal phurge respected wrote the octomer been boneralim weory if\n",
      "================================================================================\n",
      "Average loss at step 33500: 1.580457 learning rate: 0.010000\n",
      "Average loss at step 34000: 1.577014 learning rate: 0.010000\n",
      "================================================================================\n",
      "qer on ballivent eventies sections develies defyrrafortawian can been seven eoch\n",
      "qan dectionistronicia is dia four and subjectage its centerition citar or alon i\n",
      "hed their extragloconism ty due ii and leginerenting apoky prime aucordy in one \n",
      "jated that dwanuoning scanably at both which designed without not alqomit its to\n",
      "ed fligh alworare involved them escraem on and the originate for lun concamand t\n",
      "================================================================================\n",
      "Average loss at step 34500: 1.602238 learning rate: 0.010000\n",
      "Average loss at step 35000: 1.579505 learning rate: 0.010000\n",
      "================================================================================\n",
      "dee ne which close lineral at the meased a peoplemr corne prigution two zero zer\n",
      "hed on one nine nine zero zero zero zero kidge propertant rema fouglen i fanture\n",
      "y the hold case m servations the creciple there appeared bromlogy be two zero ze\n",
      "ood jecre more medig appeial emorganated for the flown to the direch foot asmier\n",
      "nied as build but this latered as a an autons comoxaing has been pacere one nine\n",
      "================================================================================\n",
      "Average loss at step 35500: 1.588640 learning rate: 0.010000\n",
      "Average loss at step 36000: 1.584342 learning rate: 0.010000\n",
      "================================================================================\n",
      "flos dust the fanto ausist in the one zero three three the war plane discussorvi\n",
      " on three this inst one name abelling wry that ruphiings throma equicunised surp\n",
      "ful vitia and libred person r teguice of the economick the hundreed caltions par\n",
      "y highes one nine six three zero undervijally german for a hoal adama to is even\n",
      "h paircsworia chocteliages effects of by the irray also can mement and humatifif\n",
      "================================================================================\n",
      "Average loss at step 36500: 1.579184 learning rate: 0.010000\n",
      "Average loss at step 37000: 1.592656 learning rate: 0.010000\n",
      "================================================================================\n",
      "on taugeral claiake all ad to hellibateg thational kreed an alluw have openicle \n",
      "ince of thraideral it is an abpres avounas vort bastrouloch wich line three one \n",
      "bre wold speial are padmr rai seven jistorian sectical ununderrale crimdive are \n",
      "wered of calsor zero perfil rather tamf in humon and study albble the lando of p\n",
      "ntalen without as genentially building is deminish the rating his prefeet rivold\n",
      "================================================================================\n",
      "Average loss at step 37500: 1.602800 learning rate: 0.010000\n",
      "Average loss at step 38000: 1.592613 learning rate: 0.010000\n",
      "================================================================================\n",
      "qet poctogy and suffess and being to two zero zero k nati grows ksew to real car\n",
      "b austwisonal viase acculaped barked after chan a well united mainel a pately us\n",
      "provian polation notu matti and conseaded yeap storing unduling the tihrent was \n",
      "ing the nature as urdise europe s holling comment proampal biirs frouds half one\n",
      "be responsort itariated supportum the of as people me stean in lice begar austre\n",
      "================================================================================\n",
      "Average loss at step 38500: 1.601176 learning rate: 0.010000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-025ec8c552fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m       \u001b[1;31m#print((feed_dict[train_data[i]]).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_lab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 50001\n",
    "summary_frequency = 500\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "      #print((feed_dict[train_data[i]]).shape)\n",
    "    \n",
    "    _, l, predictions, lr,train_lab = session.run([optimizer, loss, train_prediction, learning_rate,train_labels], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      \n",
    "      #print(train_lab)\n",
    "      #print(labels.shape[0])\n",
    "      #print('Minibatch perplexity: ',np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      if step % (summary_frequency * 2) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "          feed[0,] =np.random.randint(0,27)\n",
    "          sentence = id2char(feed[0])\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction=sample_prediction.eval({sample_input:feed})\n",
    "            k = sample(prediction)\n",
    "            k=characters2(k)[0]\n",
    "            feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "            feed[0,] = char2id(k)\n",
    "            sentence += k\n",
    "            #feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "            #feed[0,] = np.argmax(prediction)\n",
    "            #print(feed.shape)\n",
    "            #sentence += id2char(feed[0,])\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      #for _ in range(valid_size):\n",
    "       # b = valid_batches.next()\n",
    "        #predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        #valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      #print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
