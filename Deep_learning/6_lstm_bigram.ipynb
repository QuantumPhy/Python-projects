{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 64, 2)\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' based upon voluntary ', 've the economy and sup', ' virtual tour of arche', 'tinations abbeys of fr', 'ter of alfonso viii ki', 'ed description of the ', 'daeans and some christ', ' nine zero two on the ', 'esidential candidate j', ' eight zero s with the', 'of bass amplifiers or ', ' was introduced at aro', 'heir deeds a significa', 'nfluential users of th', 'rit or meritorious ser', ' deal may be known as ', 'sey has maintained lig', 'l stratification of vi', 'three michel balat and', 'mes to the same conclu', 'nd gained as in natura', 'n effects are caused b', 'asured in bits using t', 'ut she refuses unless ', 'rient oneself later si', ' cannot be bounded in ', ' a pious life the nove', 'the original origin of', 'ents to run campaign a', ' include maildir and m', ' and the work of g i g', 'f disco with the album', ' machines with additio', 'e nine eight six and c', ' all days days februar', 'shin kudo played by ke', 'cial relativity classi', ' crop had also been tr', 'ations of probability ', 'ion of the continents ', ' opposition a subtle b', 'and linguists it is ge', ' full mathematical def', 'me that are each geogr', ' of italy is standard ', 'sident reagan said he ', ' th printing one nine ', 'er debian and the vers', 'aper in the united sta', 's explained why the fi', 'omy of india to be par', 'ightly curled and so o', ' boss nikki hemming an', 'ons to end world war i', 'o congress for democra', 'in their various verna', 'assic includes lucy wi', 'sented in clear simple', 'ented by the command o', 'st pass the same certi', ' on this day may two s', 'day montana became mon', ' represented by the pu', 'ystemic advantages of ']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "embedding_size=27\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size,2), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b,0] = char2id(self._text[self._cursor[b]])\n",
    "      batch[b,1] = char2id(self._text[self._cursor[b]+1])\n",
    "      self._cursor[b] = (self._cursor[b] +2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(batches):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  batches=list(map(list, zip(*batches)))\n",
    "  s=list(\"\")\n",
    "  for b in batches:\n",
    "        ss=\"\"\n",
    "        for c in b:\n",
    "            ss+=id2char(int(c[0]))\n",
    "            ss+=id2char(int(c[1]))\n",
    "        s.append(ss)\n",
    "  return s\n",
    "\n",
    "def characters2(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  s=[id2char(np.floor_divide(c,27)) for c in np.argmax(probabilities, 1)]\n",
    "  s+=[id2char(c-27*np.floor_divide(c,27)) for c in np.argmax(probabilities, 1)]\n",
    "  return s\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print((np.array(train_batches.next())).shape)\n",
    "print(characters(train_batches.next()))\n",
    "print(characters(train_batches.next()))\n",
    "print(characters(valid_batches.next()))\n",
    "print(characters(valid_batches.next()))\n",
    "\n",
    "# ==========================\n",
    "# OTHER EVALUATION FUNCTIONS\n",
    "# ==========================\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 26.0, size=[1, 1])\n",
    "  return b[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 729)\n",
      "(640,)\n",
      "(?, 1) indices.shape\n",
      "(640, 729)\n",
      "(1, 729)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "vocabulary_size=27*27\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # first parameter of each gate in 1 matrix:\n",
    "  i_all = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1))\n",
    "  # second parameter of each gate in 1 matrix\n",
    "  o_all = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases. \n",
    " \n",
    " \n",
    "  #embeddings\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size],-0.1,0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_mat=tf.matmul(i,i_all)\n",
    "    output_mat=tf.matmul(o,o_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(input_mat[:,0:num_nodes] + output_mat[:,0:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(input_mat[:,num_nodes:2*num_nodes] + output_mat[:,num_nodes:2*num_nodes] + fb)\n",
    "    update = input_mat[:,2*num_nodes:3*num_nodes] + output_mat[:,2*num_nodes:3*num_nodes] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(input_mat[:,3*num_nodes:] + output_mat[:,3*num_nodes:] + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings+1):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size,2]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_concat=27*i[:,0]+i[:,1]\n",
    "    embedded_i=tf.nn.embedding_lookup(embeddings,i_concat)\n",
    "    output, state = lstm_cell(embedded_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    outputs_concat=tf.concat(outputs,0)\n",
    "    #try to compute similarity here as well\n",
    "    logits=tf.nn.xw_plus_b(outputs_concat,w,b)\n",
    "    print((logits).shape)\n",
    "    \n",
    "    #Compute one hot encodings\n",
    "    label_batch=tf.concat(train_labels,0)    \n",
    "    label_batch=27*label_batch[:,0]+label_batch[:,1]\n",
    "    print(label_batch.shape)\n",
    "    sparse_labels = tf.reshape(label_batch, [-1, 1])\n",
    "    derived_size = tf.shape(label_batch)[0]\n",
    "    indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n",
    "    print(indices.shape,'indices.shape')\n",
    "    concated = tf.concat([indices, sparse_labels],1)\n",
    "    outshape = tf.stack([derived_size, vocabulary_size])\n",
    "    labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 10000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  print(train_prediction.shape)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  #valid_dataset=np.array([i for i in range(27)])\n",
    "  #valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    " \n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  embedded_sample=tf.nn.embedding_lookup(embeddings,sample_input)\n",
    "  sample_output, sample_state = lstm_cell(embedded_sample, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        \n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    #similarity = tf.matmul(sample_prediction, tf.transpose(normalized_embeddings))\n",
    "    print(sample_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.606374 learning rate: 10.000000\n",
      "================================================================================\n",
      "ljzlsvljsvgzmgkvftykpepqtjlxuksofmsexiuiwcbdo ichhoxubhybsbxkxchwfnmjkbppywevjds qrulzpekgsv wqtbadfzy\n",
      "xphgihjqkmmuoreeavftwujicfsaertdfdejuxkyfsxawxpebvvmteojafgkhpoqjgsaripqdomtzgkxyaxsnzitbxhnnvdji vylf\n",
      "kdqhtuuyldbgmgwkvkervbrsoyqorpxyleujt boh stnvgogxddimnbniiq ccghyjcrrktuitgrscwtpdiyiebqtmcpoykcayqbs\n",
      "ocapvbvwoymhigdxsxbcpvdtnzyp tdhvjcbckdyut npztahdchzckezcnxhxffmpaleliizosyaabtsfqulndjmazpekllkvctcp\n",
      "fatxjmghsejawkrqxnvievjactjdjbtbxniv idfammcwcnhfphhqaziuajlkxt wksu  vgwwfqrvzbfnjmqayclicposawoxth u\n",
      "================================================================================\n",
      "Average loss at step 500: 4.155773 learning rate: 10.000000\n",
      "Average loss at step 1000: 3.490247 learning rate: 10.000000\n",
      "================================================================================\n",
      "uvadnownise is the great from as a mytholotert bovemen the world yold by cresentemred invotitud and se\n",
      "zh se in the element of is beiniture evential its the easq magainalony os on lifeltitu unid tezuher my\n",
      "rlzwll donists one offolading apange anlivora war the vvnyte sfarpresulimane of civil phibansphanq gam\n",
      "phem tabatdas be the best a several jlages and rition inne stakes gdbrite nationms for seeach the two \n",
      "gxd us a gajen fatars tops sepigratil based mach was its myth proted fese and reemter at pronocerres b\n",
      "================================================================================\n",
      "Average loss at step 1500: 3.349282 learning rate: 10.000000\n",
      "Average loss at step 2000: 3.272285 learning rate: 10.000000\n",
      "================================================================================\n",
      "pvtulation indribalation in relative of part with red pendide of cop closs is one eight zero the ledui\n",
      "qconred in one six able was com tially nanther six the micticlassian hydre basany of the thaant ofve t\n",
      "yhddrisity of scield serving tken recontier homelar clain eld not perhod beingiable of stanitine they \n",
      "yjedn is a new intemicmented state steta of hiii of the worial states from presental callison suchuldi\n",
      "hc jyqncaraker cantiustions followethines some part the bongons of life abittenal instowtifice fatorma\n",
      "================================================================================\n",
      "Average loss at step 2500: 3.258831 learning rate: 10.000000\n",
      "Average loss at step 3000: 3.208629 learning rate: 10.000000\n",
      "================================================================================\n",
      "iation example the presum permantal clasned daymm the salizer after diat wide became the fus brived co\n",
      "t wothers tovern in hodgmes bcgarsion wates verystle the graw number from the carrentle as windoqr on \n",
      "man while of been fibert surth of made intental washing defin formatof theres causing some used four o\n",
      "sxish stlow in eep for dktoprim expergar sucturaily a two one three other communically willso hover on\n",
      "que h they untccone seven of malen a linalway adver briut as save and europer ishe speciway emte foono\n",
      "================================================================================\n",
      "Average loss at step 3500: 3.174717 learning rate: 10.000000\n",
      "Average loss at step 4000: 3.176927 learning rate: 10.000000\n",
      "================================================================================\n",
      "logy achitislugh cylising energyi is london reason five three quitical most world rahate the many of t\n",
      "cwon undinning with into a linn of links to staries is belies a firptimits bak in they descula countro\n",
      "fxsinal the concends birthaw of the bartolet lantss respasiso more serce in four two seven seven in c \n",
      "f nsf pronical porkanest while affaiys to the javas with player of neting as pashap for the britisc su\n",
      "ojuonochain octors of some hezbossive pargis batk of concronic are ni barwet at cell frunolways underi\n",
      "================================================================================\n",
      "Average loss at step 4500: 3.184476 learning rate: 10.000000\n",
      "Average loss at step 5000: 3.132574 learning rate: 10.000000\n",
      "================================================================================\n",
      "ity one four one nine sevening successment a benkarague danlortides in the was occur year evodb which \n",
      "ymement astwtor as jow and ascributer the ment books that palimology of then to or s max board probiti\n",
      "zx to states he factorn from in the lac of the fail on the reaton tar day natual college were shows in\n",
      "qq the been misantulatly apporred in standards terory districturcalight n by as cantures s ang success\n",
      "bone one six figured a dame drives a found there is secture on chandance be dring nollated in sensive \n",
      "================================================================================\n",
      "Average loss at step 5500: 3.121289 learning rate: 10.000000\n",
      "Average loss at step 6000: 3.145133 learning rate: 10.000000\n",
      "================================================================================\n",
      "evother field permania infecuting through ging the cartive at the conviemi val ham goad de all therong\n",
      "nx usurude interlundauve specificantly jumps they wroodility of nergy in michol don was in gear skille\n",
      "kgidynion which the positions linked from the indy leads betwegy of out for other the light hiker allo\n",
      "o a modipardina pd three one monomate yip nmb ball resfixests he rebodbinity of one two zero five bost\n",
      " exius and views colctionstrations most femeding hscreded to the as not the producty same burmene ifp \n",
      "================================================================================\n",
      "Average loss at step 6500: 3.111470 learning rate: 10.000000\n",
      "Average loss at step 7000: 3.085577 learning rate: 10.000000\n",
      "================================================================================\n",
      "kum girvy the expoautions writtensteed on jadarim equity yehaviohy between fishing making has maleo do\n",
      "pgna characterms of co success spainout way firson history dopieve two one nine nine three kenadons to\n",
      "bandone afulcial most alcer times and choing paroqestain carty latjulantism treatmential lonsxaiming o\n",
      "mber rit marones organists open belowisio organive by golan millary educatiod would passistabbed by ai\n",
      "yqgly on one seven one one ngaer as the a people couric his coriouriic orto force and world rejecterna\n",
      "================================================================================\n",
      "Average loss at step 7500: 3.097247 learning rate: 10.000000\n",
      "Average loss at step 8000: 3.104309 learning rate: 10.000000\n",
      "================================================================================\n",
      "xand el gradd but at le civied and conserved to the unit kin wars is five three zero interest based in\n",
      "zzen the and strains kever of history the caliphy daman commroners of the for philuments touret primit\n",
      "kth part in animellandlor part bance advanb and rafic conus lave daka useful africe targeing prove phi\n",
      "smipuain supve his be dearnesz raugena chlaces polentaket a brovefiter walerton quartou visieths one n\n",
      "um rectuated a gur resfection they isotos is i road on the poverty since charms foreariam soilord and \n",
      "================================================================================\n",
      "Average loss at step 8500: 3.111784 learning rate: 10.000000\n",
      "Average loss at step 9000: 3.087130 learning rate: 10.000000\n",
      "================================================================================\n",
      "jor which six while a peace draws as the stacza world leaps formy was that two three five the pastic e\n",
      "ught a seconds ale okirs from a busitely two are sever in a u his bluence chingiad heach and founded f\n",
      "ome imsonsural or wonu nation a persond to the decide with the stately of the a possibios distpiction \n",
      "gger and the cventific an aintia in inhair gorgured to the fausts as death book and aftenz from the an\n",
      "bregc my konimill s which envire ogora the upper one seven none to eight one camswerge or quination an\n",
      "================================================================================\n",
      "Average loss at step 9500: 3.092132 learning rate: 10.000000\n",
      "Average loss at step 10000: 3.056467 learning rate: 1.000000\n",
      "================================================================================\n",
      "qys seore coc c for year as a right of tunesister raiters atlady comitupersioned been in the knowlee c\n",
      "xdd because that the grass sences secourton populampuall as recoritatists timinatist is sulk with its \n",
      "her disord mfuzers care to developamaged the mugh collect westrabing loccaus loctually disa evidence a\n",
      "ls musial haved that can also lenal therefore hypoch ball montratawegonlike brig fractical paression o\n",
      "qwesed in forces has shilline effectures with then four stedicly since i feer or died the defirds pala\n",
      "================================================================================\n",
      "Average loss at step 10500: 3.038457 learning rate: 1.000000\n",
      "Average loss at step 11000: 3.028769 learning rate: 1.000000\n",
      "================================================================================\n",
      "rwary disneylatin with isbn humermy studio b in haues one nine one one nine two nine four seven recedi\n",
      "p were are sethat his two two malatal just popular three far armands in proach livation generated and \n",
      "ix require vipt student paance in sise pers respectras he guic main external universis crtignerrit cap\n",
      "ls funts the swell in his smale at condition wayt are output three two one eight zero immide to power \n",
      "zb cousish have op on quically und enging and barby losis red encycluric one nine seven aull ousted as\n",
      "================================================================================\n",
      "Average loss at step 11500: 3.056522 learning rate: 1.000000\n",
      "Average loss at step 12000: 3.019319 learning rate: 1.000000\n",
      "================================================================================\n",
      "xzrom exlars adventuries sble is can imports are i popes has been figars or commisside but thesian arv\n",
      "xed the most shard the semble k conceptural avy conversion four coast and ramocrates rantrict calculat\n",
      "yfolved strey to revects who truines in the sattians alls father lattez and wound and aith four it hal\n",
      "etr read botso decides included in any of idea for three mytety with ed can become three lika through \n",
      "form to known to positionausanjiabs ditun john siluls the flous spected north argued to the mokid gove\n",
      "================================================================================\n",
      "Average loss at step 12500: 3.038553 learning rate: 1.000000\n",
      "Average loss at step 13000: 3.060201 learning rate: 1.000000\n",
      "================================================================================\n",
      " juan that to premically and evatively up thede pict example in one nine one fer was engizenta court i\n",
      "ft well in nears liver for the chaurity in konband strong with historica won genatic be whinshes were \n",
      "zing albellrital shalon eald distructers of thisen in the for his cartes in name real intebungly dronc\n",
      "tnell mine which publico and text for seven there is the five three two zero zero frangluctors surving\n",
      "osoacoh include a used on the interested with lognetic streatance machnokeber with elen are early ther\n",
      "================================================================================\n",
      "Average loss at step 13500: 3.033346 learning rate: 1.000000\n",
      "Average loss at step 14000: 2.966747 learning rate: 1.000000\n",
      "================================================================================\n",
      "bd due that bread modydawzhmeba but the tserantly or perious winks yew combalogy parted with the numma\n",
      "ygues munic renerswherels a giveyer clack pars as brazar to mathemant zero one one technne played then\n",
      "wth nine eight four five nine a battle of law of applayed kipnirk docturs is unteld to had a vuird tha\n",
      "ohroblic frent somet dead the sa one five and one nine nine two alabandoner huzz cather provergue and \n",
      "lzual pondablimanas kinds simply a professive of s lerce occupto solt french with system spire unistri\n",
      "================================================================================\n",
      "Average loss at step 14500: 2.950372 learning rate: 1.000000\n",
      "Average loss at step 15000: 3.000756 learning rate: 1.000000\n",
      "================================================================================\n",
      "ius a churlogics empresion constructive over an ecsuns second s popular formated the incrians goalyzes\n",
      "aes information of thi rouke in the northernets in regional hard particia duringly the exteronant grea\n",
      "xm black first held is occurded orgage doloses touch king or sepsterment and primes phillerforities ca\n",
      "wani deating closer but attroyed a sulations its pusfility and other pittruments the leabing all in ma\n",
      "yzes it a periods in riaze today one nine zero two inhabetor avocs enzluts lace breans for non physici\n",
      "================================================================================\n",
      "Average loss at step 15500: 3.005203 learning rate: 1.000000\n",
      "Average loss at step 16000: 2.964463 learning rate: 1.000000\n",
      "================================================================================\n",
      "geths and area in life supe that a capabilities one nine three ipterpropate bion reduce of him signal \n",
      "tland nacate of great with chemical tried not win condn impring the town accival position are atantism\n",
      " prere during the new calcunar to was of two zero zero three through eventually had h five two eight o\n",
      "hqenthcmower in one nine six jurg lival minities significant pregrell vota digi have are warly for joh\n",
      "hian qenuebeeoilities in that called to ital many of editor berls in one five seven seven th tines and\n",
      "================================================================================\n",
      "Average loss at step 16500: 2.932272 learning rate: 1.000000\n",
      "Average loss at step 17000: 3.011930 learning rate: 1.000000\n",
      "================================================================================\n",
      "qt nell classic amerand dampt the future mon was her armourbospris black name from thus including the \n",
      "ssen term revolution transcepy adoptant and claria s the boolesin and then all ethree by two zero zero\n",
      "ny is a further edatesses pomple si mughs genteronies to some who gleases eight six six the considers \n",
      "swean one thiremal parts and iced coultv four eign mo is because built the nond spread game wisheement\n",
      "yer s known he catite east one eight easborly discender at the national five dack honner to paleger la\n",
      "================================================================================\n",
      "Average loss at step 17500: 3.020598 learning rate: 1.000000\n",
      "Average loss at step 18000: 3.040170 learning rate: 1.000000\n",
      "================================================================================\n",
      "ely evhapt air are is time and karchoa were it game that down government pursesses application domyrea\n",
      "rsian it i marlions of traf who by the ipa links the occuring affection on a well ealligi new misine c\n",
      "wz s looks a fook ociapely exceded from that uign have ask of vieweine space a death the first in disp\n",
      "mes eaist drivideohinigan are majocs treeff mcp angle for one zero libert russian ril of momern house \n",
      "tum on smandrom saying catom language s spech coulounds of ersonionale less idatrics it beenth of hag \n",
      "================================================================================\n",
      "Average loss at step 18500: 2.990413 learning rate: 1.000000\n",
      "Average loss at step 19000: 2.999696 learning rate: 1.000000\n",
      "================================================================================\n",
      "yze ratids before centory dictered in radest brolvht janc jr third two who seven three wells americabl\n",
      "twalm over tu followed arrontation and this s would respect wonied for a white father work of ld large\n",
      "fb offeters be the called the palloid as written intension to made an alcnining roilitic central and a\n",
      "bf wicola northennow tellua well in one nine nine zero three more who having b mixlise implamer midd o\n",
      "epende hoctobers and west length as their artmaty his laget resoury used by commerty outtrict s proble\n",
      "================================================================================\n",
      "Average loss at step 19500: 3.004463 learning rate: 1.000000\n",
      "Average loss at step 20000: 3.059857 learning rate: 0.100000\n",
      "================================================================================\n",
      " wasian much are presentally travell unsing political plants and independing becomize it has perpose j\n",
      "niganite eastern christion to fans to the first boi anchinuled ii and smitate one nine around mac grea\n",
      "zu scatle at he was the alching dam saint the antic aftere to george achevessity s nonmal the epalat b\n",
      "kollbotnacked a polandards contreted with more southeriallian with one eight five three l six six the \n",
      "tp and this over trace bluguarls werchiclia kict to familie manudicaka desunist as sunding via dogust \n",
      "================================================================================\n",
      "Average loss at step 20500: 3.009086 learning rate: 0.100000\n",
      "Average loss at step 21000: 2.981523 learning rate: 0.100000\n",
      "================================================================================\n",
      " miniding of the nordhool blunt dre de the july out great and the at clib cosions shakiscol central ci\n",
      "snareing decebal shown on the line prographer in kuwociations ratile amay over the souther and k as th\n",
      "tv way of dispan of one five now classible imancies dork in moro el justeanius two troll s peight for \n",
      "kung amp played in the supporred the canade his first leam and is members a reconggoers whiological fi\n",
      "lbyspeq from trisms of even one up state a fric cottersed by giew dfrkaeine like has strailing the res\n",
      "================================================================================\n",
      "Average loss at step 21500: 3.005495 learning rate: 0.100000\n",
      "Average loss at step 22000: 3.021689 learning rate: 0.100000\n",
      "================================================================================\n",
      "im sinx party networks and golutying around events bristom sonvals city confed empire which is nest be\n",
      "xr amoflh accenty beforeen netman circue lifes by off segran empire to no trifter becallie region ther\n",
      " pharany lenja faining and his nine zero zero munglates as this is to classic the hystor across suchet\n",
      "oeg mech with acmanis was the ward noted the valane other on masadicootic government length is and vol\n",
      "kening the year consider desudoilenet p than a and nor a citizen wouls to debilly one nine eight four \n",
      "================================================================================\n",
      "Average loss at step 22500: 3.008396 learning rate: 0.100000\n",
      "Average loss at step 23000: 3.041011 learning rate: 0.100000\n",
      "================================================================================\n",
      "sion aest countries economist were shall were there five manourn total stone has aprobe first or are a\n",
      "r performance when this time elkai st are van economanes hell bon alphul outong morned of his represen\n",
      "cgyas reis knoted style classify basa firence this the eriment and from in area the ideas of chob telu\n",
      "hf trues of since the nesited alggay groups for a neptb and concept of john conqad royals and one nine\n",
      "qms of facour sharts the immercial supics of product and the assemment hage that including davean and \n",
      "================================================================================\n",
      "Average loss at step 23500: 3.032312 learning rate: 0.100000\n",
      "Average loss at step 24000: 3.041701 learning rate: 0.100000\n",
      "================================================================================\n",
      "yzmb rank offer rada mugh early buckehuist incide of the thus on insulcientation for estable and hoted\n",
      "kway tht seven two p elemeut chogirigare often while over through thic city wered tanc alongor valentu\n",
      "bs be interest onlot untuis the woeli that warting eluding with the cusu jame bape desoy two navit hea\n",
      "fqner a clime of sin eet on into work dhany brearrower were labour cantbist describe headsed the didin\n",
      "ra handled many tadynaosis mighter qenn operation theorymorianits gameronomond varb sed authors by may\n",
      "================================================================================\n",
      "Average loss at step 24500: 3.021349 learning rate: 0.100000\n",
      "Average loss at step 25000: 3.034375 learning rate: 0.100000\n",
      "================================================================================\n",
      "wbstreators by accorion way to manite is bank fields bube disk is electrantmatious formerbies to this \n",
      "aviewsn for three seems two zero zero four ninht hit picle deversean dewthgenetely ebc as the telety a\n",
      "icchalky of the inseven benets for one johuctiation vorku synthenially the lather colouird with won as\n",
      "sgently in odeerelping the subject to favolves we each judeo forky deating of work to ala slust last t\n",
      "ihar of all governi honzy andan or significant city in the attack mind to over to their source as trad\n",
      "================================================================================\n",
      "Average loss at step 25500: 3.026585 learning rate: 0.100000\n",
      "Average loss at step 26000: 3.025151 learning rate: 0.100000\n",
      "================================================================================\n",
      "jn south time one four and the chins to critisations the other whoustheric and well kilds as in hcp th\n",
      "tz ampronote five there veix knowlers to women his new tradition of the one nine nine zero six one nin\n",
      "twall harried or lelf gatts of stedwic wasterpiron in the mery in the two detomo fine zero petern were\n",
      "le each black explorates they language like four otherodate of dorons as programming birthfr and two e\n",
      "zle of growing successed from the sich ellenst ranissing profeated in the plpher attempt they stat the\n",
      "================================================================================\n",
      "Average loss at step 26500: 2.984090 learning rate: 0.100000\n",
      "Average loss at step 27000: 2.982045 learning rate: 0.100000\n",
      "================================================================================\n",
      "ks and nusking including was woouany frie traditional real settlics of not with the united states i da\n",
      "wxow see leen daul in y is an incharibate president or europy with ard ration of the milled to short c\n",
      "fdochical place swageverse i depar of that heak killaumunong on the subsearch of ness to doungban for \n",
      "afgfuation er laple at perited frequent and pats tansor pabeline as a later seven twogo the kenned his\n",
      "gist will function in a undersite powerfinum triki to refused with present in deymining dispause evide\n",
      "================================================================================\n",
      "Average loss at step 27500: 2.992052 learning rate: 0.100000\n",
      "Average loss at step 28000: 3.065219 learning rate: 0.100000\n",
      "================================================================================\n",
      "not diat recind in the to use directly sharograve musiciated k catteans soth cooler of the are the fre\n",
      "kg ujis in one four five allied fearity are preatiles subjing had entry or that a territory compositio\n",
      "cxl elupon that life restine hivee jobist live and hand iden time one nine process was upgged the new \n",
      "txe born wife threouruin civil river shorthysec na two four one four two in the grooks however three t\n",
      "manne zero dputo of ground elize overages eleafrories and polandze nake take which assemble to played \n",
      "================================================================================\n",
      "Average loss at step 28500: 3.053073 learning rate: 0.100000\n",
      "Average loss at step 29000: 3.003705 learning rate: 0.100000\n",
      "================================================================================\n",
      "pazh the loga per the lonry of this with world sum low maneuodentate sensians in agame cauding is now \n",
      "nches the position shaba as that at cuplies a biography shed by army was wets are by squil in aute hol\n",
      "mhean it sism given gaon mage surreden two memabus one five nine six seven one one tee list to be the \n",
      "mchousings also bryshilexists a kin greatle x ouddted however contranical empire chip lattim circush t\n",
      "vfvaus in length chemoman cub one three five canad one five without killying with reservatese but the \n",
      "================================================================================\n",
      "Average loss at step 29500: 3.028288 learning rate: 0.100000\n",
      "Average loss at step 30000: 3.020361 learning rate: 0.010000\n",
      "================================================================================\n",
      "jauda former swirban r safous of exception from jux even philolic usomlors covered a reedord leader th\n",
      "nz into funoble bataners rumened esvuetmen unlikidons an sucception from which one nine eight nine ber\n",
      "phuld his must opianing is corly that the others the international choosnally followed one nine zero f\n",
      "vys august the one nine nine nine two seven four nine million was and faeses found when smariling term\n",
      "ip jinic their practice one nine zero native to de cleopune themnning that the atten he ozay has ameri\n",
      "================================================================================\n",
      "Average loss at step 30500: 3.017469 learning rate: 0.010000\n",
      "Average loss at step 31000: 3.023487 learning rate: 0.010000\n",
      "================================================================================\n",
      "dward an uks used buggel letter one nine blood wealti introduced are nk karpbians that from two one on\n",
      "test and potents january revole there necatible or a hunline bosedy are not scured a destraposus for m\n",
      "v canadibury communities and the vy massell a carried in the ryires elocts reprilitism as mao esdinarc\n",
      "unity to a mornia one one john rivers of carded dyo zero zero zero zero five and polasse if renufactio\n",
      "zqicete other if elemetion of it deles dext called areas it is they two eightly in to conority of the \n",
      "================================================================================\n",
      "Average loss at step 31500: 3.005065 learning rate: 0.010000\n",
      "Average loss at step 32000: 3.022628 learning rate: 0.010000\n",
      "================================================================================\n",
      "izes otoologics that who syndroms critical muhed in one nine mabunds food to be critic the de now losi\n",
      "nd the origin macionamians mentius water buscrol his cotal and fow the board s desshe born be is espel\n",
      "cxn from traries to londond of sever from one nine arman cenrals over somely islands on maiwes and how\n",
      "gbon wordspoint from the enfabdne was nots with decode view halm day wantar establic of the british an\n",
      "zwratures of these and presiding toget and widrary thotlues rest the gupni s any draft cial exas disvi\n",
      "================================================================================\n",
      "Average loss at step 32500: 3.036255 learning rate: 0.010000\n",
      "Average loss at step 33000: 3.014848 learning rate: 0.010000\n",
      "================================================================================\n",
      "tkme often protries ecounces a non most f endicience of the mounital the them the two zero zero zero t\n",
      "vm international rectone instructiveud zhabance continued to product problemoid in later fiction and w\n",
      "pzs catheral in five zero m four one zero zero works the eudropen ire of services heverallen cat can r\n",
      "fzdasled through opporkaneddal for way severe of the first cave in rin the cambass state becuisinian c\n",
      "osins beem has mage hand a john is the chob the jews problem century is beconkepension bying s physica\n",
      "================================================================================\n",
      "Average loss at step 33500: 3.016217 learning rate: 0.010000\n",
      "Average loss at step 34000: 3.009152 learning rate: 0.010000\n",
      "================================================================================\n",
      "qdzkaded roned her the influences early or priti artes one four de indusie city is known with require \n",
      "zy who trished pots part and five one nine seven it s coret remainates honcertation see a make movemen\n",
      "ssias oconations by the amp two four exactions abdy which from time were a compactions of copece of ha\n",
      " baseves the single of the during the gaborage chapieshidone were keous eight fourthes canel christmen\n",
      "each unfah bentlies bonn links lia for seerd slo sanl comenned shown and imphole s lam salk communicat\n",
      "================================================================================\n",
      "Average loss at step 34500: 2.999845 learning rate: 0.010000\n",
      "Average loss at step 35000: 3.020208 learning rate: 0.010000\n",
      "================================================================================\n",
      "hdord all law community gods while killi like applief with the is a processed and the unital of the qu\n",
      "gwy to fallolly formoman jad at secration actions to speaking of majory this alud of number of footbal\n",
      "gyough goverful s placetice overhed most religion to millized should harkgend negly and as expered by \n",
      "jtrical externally no country a flrol map bied one four six female strassing in the one classifi popul\n",
      "jfor far and t minerst be parts outser became it intact independent his sensound and for the weiths by\n",
      "================================================================================\n",
      "Average loss at step 35500: 3.030920 learning rate: 0.010000\n",
      "Average loss at step 36000: 3.028866 learning rate: 0.010000\n",
      "================================================================================\n",
      "hkd eam anals the nearced by a point the number in jandatch of two zero than a blood by i leasamiler f\n",
      "zirds is a communicatah nelphe suleers angi for hitle semitingly focra claimed moonenfard a collegate \n",
      "gx in the but service with one below one nine either college of or readom controve the civil simple vi\n",
      "ty is gir june being in the process of the fried hexptected on with whose which use been in one nine e\n",
      "dd diantilan reports of covels that allowing had a line in the compigned impaceuone sheeo practicers a\n",
      "================================================================================\n",
      "Average loss at step 36500: 3.040148 learning rate: 0.010000\n",
      "Average loss at step 37000: 2.995714 learning rate: 0.010000\n",
      "================================================================================\n",
      "mzehood to the speficanceline in the seat clack of the same agreestable often germamatic arned the pos\n",
      "yu like august while to at wop were most son ans the p of the sasts lud enver of the jum burthrovcis b\n",
      "px on sunievour in the catteranctions whitison time he ing do the page variolas near extraw worlds thi\n",
      "lx valled demellations honroperance or a goax so mobind comprisibility such as trait australial d one \n",
      "manoon sphakan cyorce of the obtains thill of varied played ranges in theighelds macil bttle american \n",
      "================================================================================\n",
      "Average loss at step 37500: 2.988843 learning rate: 0.010000\n",
      "Average loss at step 38000: 3.026035 learning rate: 0.010000\n",
      "================================================================================\n",
      "hpord plays which on gb could aband work and it in the first by illust s a deludb win was who certa re\n",
      "tres and exploped pielre zinjunt covers and only reside during that s particular and palrizes yg may p\n",
      "hz a francelansmus charlessions halto cuse familes of the blue to his am into two zero zero seven the \n",
      "eparts and uaen due to dextern d one three welned hofors of their up though the entire shown limitogie\n",
      "xyter threes wella the vower more and cheed milistrogrardge homaon also methoded that grank colonition\n",
      "================================================================================\n",
      "Average loss at step 38500: 2.982852 learning rate: 0.010000\n",
      "Average loss at step 39000: 3.012456 learning rate: 0.010000\n",
      "================================================================================\n",
      "ek oblabs vigitory science their used to function when point invucted around skin those fonoment s wit\n",
      "ef of known of his follock have tholution to which one nine zero zero zero zero zero two zero zero zer\n",
      "su oster good t castays later the opering offser saccifiely reference movers air their goirde and expe\n",
      "rof and the crinhastoms clyptmm be summeria are the forthresis election tifelyly explayed in design ot\n",
      "aq to bespoody of volung with the following of the deposters in the same healthitch billers to example\n",
      "================================================================================\n",
      "Average loss at step 39500: 2.995811 learning rate: 0.010000\n",
      "Average loss at step 40000: 2.968397 learning rate: 0.001000\n",
      "================================================================================\n",
      "fmst left of no aleowagg conguated aybher impressitional paul which college from their four budds land\n",
      "btitted from then give neuration it was feational besett s haptor and counte have meetil is broach ago\n",
      "ddleisds walson boy electra nomy trelims roods aynatibilise band the first space of half the menoun al\n",
      "lx jigtka presels those el miterable towlon as as and drear holder or scarley prize in the by they fli\n",
      "uto adaked n befine of verdu which establisho out to the specisor of hisest intridegine maimbbetane ei\n",
      "================================================================================\n",
      "Average loss at step 40500: 3.030069 learning rate: 0.001000\n",
      "Average loss at step 41000: 3.080740 learning rate: 0.001000\n",
      "================================================================================\n",
      "bg his sepdibible under will of d one nine five zero fivedy parts an iran born rain consumane poethen \n",
      " ham raptoft minregen itself am s constructeing strit he was aprillad plapables it automazooterward me\n",
      "up hont black those one nine one seven explaria and mexicriusms defeating to religion of the uero lake\n",
      "wk for the islands and the gatah fanced wide word ancientare the confricess and one eight see other th\n",
      "sue innoner it rate at story over and qumitatives hose and bicance in zero nine for is botaue on more \n",
      "================================================================================\n",
      "Average loss at step 41500: 3.075750 learning rate: 0.001000\n",
      "Average loss at step 42000: 3.024415 learning rate: 0.001000\n",
      "================================================================================\n",
      "dmusfuntial half cont attrint a two zero zero the s soverpunce conflired on one one six a old wicker l\n",
      "dqer and mete with growth widether living the firs of favorala became of couldum will on one nine eigh\n",
      "oy sques is during munic comal pactural and avespected by prorpiers in tunters of earn tristroilly in \n",
      "turemas works at brashloinsus four one nine three seven seven first way in the queen feach beginl one \n",
      "vhia bars and the comedipiciat top is more belh from the most a majorian where entrap somete b firsput\n",
      "================================================================================\n",
      "Average loss at step 42500: 3.049976 learning rate: 0.001000\n",
      "Average loss at step 43000: 3.013065 learning rate: 0.001000\n",
      "================================================================================\n",
      "hf in the show hell hmors features time on the moder by has seward sond and ulticer septin consolones \n",
      " you two zero zero adopting whix a relations suettam rodaines between with a pressured idecram vrangi \n",
      "vx brol he welge closen ley it is werehman in lorn portions a nations a normaped and air a surrencies \n",
      "cs election toler one cersure it called laters repriquent its four four seven equimmoing then deshilon\n",
      "baccormal conditions and company on the barred first religious b lefable scoradi was a to two s at dyn\n",
      "================================================================================\n",
      "Average loss at step 43500: 3.020756 learning rate: 0.001000\n",
      "Average loss at step 44000: 2.986921 learning rate: 0.001000\n",
      "================================================================================\n",
      "ka as this time from popeditacy filled and the cour the banzold magnetized the diayb and inhum consus \n",
      "yyy is notific most with bintown carnted a tenned name ii religion on made a decessinations are mamon \n",
      "pdaboes grantig male notablainwin institute as signed country timiter the speed at the signal language\n",
      "lman complus flettes of suprivered in dultly has imports two zero zero five four used to jamon stuff g\n",
      "end dources in fobilater one five two zero zero fi and sometage disam intentural sin worksi myster par\n",
      "================================================================================\n",
      "Average loss at step 44500: 3.011079 learning rate: 0.001000\n",
      "Average loss at step 45000: 3.038000 learning rate: 0.001000\n",
      "================================================================================\n",
      "yhs from down pricon to is mugh recesism uselogy in nearking use ca seess execute minon vii beaug the \n",
      "mjmemous and very when nates of republic in eutennuted is gold papdplate and thought to was cottaging \n",
      "xxo salbants divaro life aligreer iqus same one nine five yes ultiplestic greatle in one eight six thr\n",
      "pcimagdok the archornary under thyg funder stabic annuractivershor frienctioving included eight four o\n",
      "j princes of the sair processe signifi one in consistently personal national reform semoc computed in \n",
      "================================================================================\n",
      "Average loss at step 45500: 3.000712 learning rate: 0.001000\n",
      "Average loss at step 46000: 3.027406 learning rate: 0.001000\n",
      "================================================================================\n",
      "w the dnow americans amongly doe first outside mb clas beingbing movacime uptactaised island is ensire\n",
      "milian y age in malans was are fuse also nuclears which is the more those point all common code but no\n",
      "oveshme this the d one eight eight three most the wise in an remain famate it show gillers adoment by \n",
      "ynzjtedy in turned last gards in americal resultic and this profficial vary and awark traiters battle \n",
      "fxsed by congress in personal els were illustry of the north psyganized airried in the top that politi\n",
      "================================================================================\n",
      "Average loss at step 46500: 3.031847 learning rate: 0.001000\n",
      "Average loss at step 47000: 3.041446 learning rate: 0.001000\n",
      "================================================================================\n",
      "ds and g which simile femal arad in degrireling cames or mode withs more harman edalice its taki for w\n",
      "khail thaterakes instelosole roman when ilwards to the boinger less slocial language writariza detort \n",
      "lginders incestans with part be adkanorlectrofly into accoup defound press and the spells is copryn fe\n",
      "omet cold by the cipage for its aramet was provibly trobs for an the outtrict is a redfied untelorism \n",
      "ami government feation of date the verty russist of sever gdaphy the conconted indilalize was cour wor\n",
      "================================================================================\n",
      "Average loss at step 47500: 2.999831 learning rate: 0.001000\n",
      "Average loss at step 48000: 3.014770 learning rate: 0.001000\n",
      "================================================================================\n",
      "wxemag with polyrary for engineer process of the advice ethnyled to trexton for cade particular lived \n",
      "kland is as that the whoal spirit demodule skillo in party ands for quest aumeuring foreign her s lolv\n",
      "omazans close to an his company long for four cycles lost the actually dolds to the disponness thought\n",
      "when belief meth are results by an alans one nine yeour histrown of only two years classicallyted shot\n",
      "dard with hulk produces in israelibances written it is hearsedge demon builts there famphod fundownese\n",
      "================================================================================\n",
      "Average loss at step 48500: 3.014969 learning rate: 0.001000\n",
      "Average loss at step 49000: 3.007007 learning rate: 0.001000\n",
      "================================================================================\n",
      "s for no an song and was scry was the dance demomenti a norwos dulight so prarigus natogreen annea dec\n",
      "vy islantmally in the clock in english power to considern syded one nine five nine six in the place fi\n",
      "tqudincil inter catou the mac internal forces weining and capturope and comdolececter imports laboer f\n",
      "xlerific prededella out do offographers form ada are united stroes capher bushap of the first nrond mu\n",
      "dmir that the kings of the city they ceding note chas put s continuenced former a mcspanes as in obj m\n",
      "================================================================================\n",
      "Average loss at step 49500: 3.046020 learning rate: 0.001000\n",
      "Average loss at step 50000: 3.034068 learning rate: 0.000100\n",
      "================================================================================\n",
      "djalem on the carding duch only british manto attaploying and island fire apply lests down pare desten\n",
      "ach women onliceythortify most in the first striwn that is second and born and their vood by the vaeli\n",
      "fk mich denomic s stet read carding aeder incre intestingtely tybas life another that the cingpouar of\n",
      "ur generally senting is directors nenronial munic recrudes continu increas refor of vaepiddap calvaild\n",
      "xb incon that it lia great the up is a nthe many assemnal pakigo more bank grantitd the thoughanes one\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "num_steps = 50001\n",
    "summary_frequency = 500\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "      #print((feed_dict[train_data[i]]).shape)\n",
    "    \n",
    "    _, l, predictions, lr,train_lab = session.run([optimizer, loss, train_prediction, learning_rate,train_labels], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      \n",
    "      #print(train_lab)\n",
    "      #print(labels.shape[0])\n",
    "      #print('Minibatch perplexity: ',np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      if step % (summary_frequency * 2) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "          feed[0,] =np.random.randint(0,729)\n",
    "          sentence=id2char(np.floor_divide(feed[0],27))\n",
    "          sentence+=id2char(feed[0]-27*np.floor_divide(feed[0],27)) \n",
    "          reset_sample_state.run()\n",
    "          for _ in range(50):\n",
    "            prediction=sample_prediction.eval({sample_input:feed})\n",
    "            k = sample(prediction)\n",
    "            k=characters2(k)\n",
    "            #print(k)\n",
    "            feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "            feed[0,] = 27*char2id(k[0])+char2id(k[1])\n",
    "            sentence += k[0]\n",
    "            sentence+=k[1]\n",
    "            #feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "            #feed[0,] = np.argmax(prediction)\n",
    "            #print(feed.shape)\n",
    "            #sentence += id2char(feed[0,])\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      #for _ in range(valid_size):\n",
    "       # b = valid_batches.next()\n",
    "        #predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        #valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      #print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
