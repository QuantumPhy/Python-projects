{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.RandomClassifier object at 0x00000241702ECF28>\n",
      "accuracy: 0.491473\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.48      0.52      1481\n",
      "        1.0       0.42      0.50      0.46      1099\n",
      "\n",
      "avg / total       0.50      0.49      0.49      2580\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "accuracy: 0.613953\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.68      0.67      1481\n",
      "        1.0       0.55      0.52      0.53      1099\n",
      "\n",
      "avg / total       0.61      0.61      0.61      2580\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "accuracy: 0.583333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.64      0.64      0.64      1481\n",
      "        1.0       0.51      0.51      0.51      1099\n",
      "\n",
      "avg / total       0.58      0.58      0.58      2580\n",
      "\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "accuracy: 0.656977\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.68      0.77      0.72      1481\n",
      "        1.0       0.62      0.50      0.56      1099\n",
      "\n",
      "avg / total       0.65      0.66      0.65      2580\n",
      "\n",
      "\n",
      "GaussianNB(priors=None)\n",
      "accuracy: 0.509302\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.65      0.31      0.42      1481\n",
      "        1.0       0.46      0.78      0.58      1099\n",
      "\n",
      "avg / total       0.57      0.51      0.49      2580\n",
      "\n",
      "\n",
      "\n",
      "best accuracy: 0.656977\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "\n",
      "\n",
      "--- normalized---\n",
      "\n",
      "\n",
      "<__main__.RandomClassifier object at 0x00000241702ECF28>\n",
      "accuracy: 0.484496\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.56      0.48      0.52      1481\n",
      "        1.0       0.41      0.49      0.45      1099\n",
      "\n",
      "avg / total       0.50      0.48      0.49      2580\n",
      "\n",
      "\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "accuracy: 0.616667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.68      0.67      1481\n",
      "        1.0       0.55      0.53      0.54      1099\n",
      "\n",
      "avg / total       0.62      0.62      0.62      2580\n",
      "\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "accuracy: 0.564341\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      0.61      0.61      1481\n",
      "        1.0       0.49      0.51      0.50      1099\n",
      "\n",
      "avg / total       0.57      0.56      0.57      2580\n",
      "\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "accuracy: 0.608527\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.96      0.74      1481\n",
      "        1.0       0.73      0.13      0.22      1099\n",
      "\n",
      "avg / total       0.65      0.61      0.52      2580\n",
      "\n",
      "\n",
      "GaussianNB(priors=None)\n",
      "accuracy: 0.618992\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.69      0.68      1481\n",
      "        1.0       0.56      0.52      0.54      1099\n",
      "\n",
      "avg / total       0.62      0.62      0.62      2580\n",
      "\n",
      "\n",
      "\n",
      "best accuracy: 0.618992\n",
      "GaussianNB(priors=None)\n",
      "\n",
      "====================\n",
      "Együttes osztályozó\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as sk_eval\n",
    "from sklearn import linear_model as lin\n",
    "from sklearn import tree\n",
    "from sklearn import naive_bayes  as nb\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# normálás\n",
    "def l2_norm(x):\n",
    "    x_n = np.copy(x)\n",
    "    for i in range(x[:,0].size):\n",
    "        n = np.linalg.norm(x[i,:])\n",
    "        if n>0:\n",
    "            x_n[i,:] = x[i,:]/n\n",
    "    return x_n\n",
    "    \n",
    "# adatok szétválasztása tanító- és tesztadatokká\n",
    "data = np.genfromtxt('person.txt')\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(data[:,1:], data[:,0], test_size = 0.33, random_state=3)\n",
    "\n",
    "# véletlen osztályozó, minden egyedet véletlenszerûen a nullás vagy egyes osztályba sorol\n",
    "class RandomClassifier:\n",
    "  def fit(self, x, y):\n",
    "    return\n",
    "  def predict(self, x):\n",
    "    return np.round(np.random.rand(x[:,0].size))\n",
    "\n",
    "# az egyszerû kezelés miatt az osztályozókat egy tömbbe tesszük\n",
    "# fontos, hogy a fit és predict függvények azonos módon hívhatóak legyenek\n",
    "# (azonos interface-t implementáljanak)\n",
    "models = []\n",
    "models.append(RandomClassifier())\n",
    "models.append(neighbors.KNeighborsClassifier())\n",
    "models.append(tree.DecisionTreeClassifier())\n",
    "models.append(lin.LogisticRegression())\n",
    "models.append(nb.GaussianNB())\n",
    "\n",
    "# minden modellre elvégezzük a tanítást (fit),\n",
    "# majd a tesztadatokhoz jósolunk (predict)\n",
    "# a jósolt eredményeket kiértékeljük (accuracy_score)\n",
    "best_acc = 0\n",
    "for model in models:\n",
    "  print(model)\n",
    "  model.fit(train_features, train_labels)\n",
    "  pred = model.predict(test_features)\n",
    "  act_acc = sk_eval.accuracy_score(y_pred = pred, y_true = test_labels)\n",
    "  print(\"accuracy: %f\" %(act_acc))\n",
    "  print(sk_eval.classification_report(y_pred = pred, y_true = test_labels))\n",
    "  print()\n",
    "  if (act_acc > best_acc):\n",
    "    best_acc = act_acc\n",
    "    best_pred = pred\n",
    "    best_model = model\n",
    "print()\n",
    "print(\"best accuracy: %f\" %(best_acc))\n",
    "print(best_model)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"--- normalized---\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "# a tanító- és a teszadatokat is normalizálni kell,\n",
    "# különben teljesen semmitmondó eredményeket kapunk\n",
    "norm_train_features = l2_norm(train_features)\n",
    "norm_test_features = l2_norm(test_features)\n",
    "\n",
    "best_acc = 0\n",
    "for model in models:\n",
    "  print(model)\n",
    "  model.fit(norm_train_features, train_labels)\n",
    "  pred = model.predict(norm_test_features)\n",
    "  act_acc = sk_eval.accuracy_score(y_pred = pred, y_true = test_labels)\n",
    "  print(\"accuracy: %f\" %(act_acc))\n",
    "  print(sk_eval.classification_report(y_pred = pred, y_true = test_labels))\n",
    "  print()\n",
    "  if (act_acc > best_acc):\n",
    "    best_acc = act_acc\n",
    "    best_pred = pred\n",
    "    best_model = model\n",
    "print()\n",
    "print(\"best accuracy: %f\" %(best_acc))\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.626744\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      0.68      0.68      1481\n",
      "        1.0       0.56      0.55      0.56      1099\n",
      "\n",
      "avg / total       0.63      0.63      0.63      2580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# saját kód\n",
    "from collections import Counter\n",
    "\n",
    "models=[]\n",
    "models.append(neighbors.KNeighborsClassifier())\n",
    "models.append(tree.DecisionTreeClassifier())\n",
    "models.append(lin.LogisticRegression())\n",
    "models.append(nb.GaussianNB())\n",
    "pred=[]\n",
    "for model in models:\n",
    "  model.fit(train_features, train_labels)\n",
    "  pred.append(model.predict(test_features))\n",
    "    \n",
    "pred_t=list(map(list, zip(*pred)))\n",
    "final_pred=[]\n",
    "for val in pred_t:\n",
    "    max_pred=Counter(val)\n",
    "    final_pred.append(max_pred.most_common(1)[0][0])\n",
    "\n",
    "act_acc = sk_eval.accuracy_score(y_pred = final_pred, y_true = test_labels)\n",
    "print(\"accuracy: %f\" %(act_acc))\n",
    "print(sk_eval.classification_report(y_pred = final_pred, y_true = test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
