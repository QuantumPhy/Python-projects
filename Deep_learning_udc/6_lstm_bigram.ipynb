{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 64, 2)\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "embedding_size=27\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size,2), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b,0] = char2id(self._text[self._cursor[b]])\n",
    "      batch[b,1] = char2id(self._text[self._cursor[b]+1])\n",
    "      self._cursor[b] = (self._cursor[b] +1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(batches):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  batches=list(map(list, zip(*batches)))\n",
    "  s=list(\"\")\n",
    "  for b in batches:\n",
    "        ss=\"\"\n",
    "        for c in b:\n",
    "            ss+=id2char(int(c[0]))\n",
    "            #ss+=id2char(int(c[1]))\n",
    "        s.append(ss)\n",
    "  return s\n",
    "\n",
    "def characters2(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  s=[id2char(np.floor_divide(c,27)) for c in np.argmax(probabilities, 1)]\n",
    "  s+=[id2char(c-27*np.floor_divide(c,27)) for c in np.argmax(probabilities, 1)]\n",
    "  return s\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print((np.array(train_batches.next())).shape)\n",
    "print(characters(train_batches.next()))\n",
    "print(characters(train_batches.next()))\n",
    "print(characters(valid_batches.next()))\n",
    "print(characters(valid_batches.next()))\n",
    "\n",
    "# ==========================\n",
    "# OTHER EVALUATION FUNCTIONS\n",
    "# ==========================\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 26.0, size=[1, 1])\n",
    "  return b[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 729)\n",
      "(640,)\n",
      "(?, 1) indices.shape\n",
      "(640, 729)\n",
      "(1, 729)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "vocabulary_size=27*27\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # first parameter of each gate in 1 matrix:\n",
    "  i_all = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1))\n",
    "  # second parameter of each gate in 1 matrix\n",
    "  o_all = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases. \n",
    " \n",
    " \n",
    "  #embeddings\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size],-0.1,0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_mat=tf.matmul(i,i_all)\n",
    "    output_mat=tf.matmul(o,o_all)\n",
    "    \n",
    "    input_gate = tf.sigmoid(input_mat[:,0:num_nodes] + output_mat[:,0:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(input_mat[:,num_nodes:2*num_nodes] + output_mat[:,num_nodes:2*num_nodes] + fb)\n",
    "    update = input_mat[:,2*num_nodes:3*num_nodes] + output_mat[:,2*num_nodes:3*num_nodes] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(input_mat[:,3*num_nodes:] + output_mat[:,3*num_nodes:] + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings+1):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size,2]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_concat=27*i[:,0]+i[:,1]\n",
    "    embedded_i=tf.nn.embedding_lookup(embeddings,i_concat)\n",
    "    output, state = lstm_cell(embedded_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    outputs_concat=tf.concat(outputs,0)\n",
    "    #try to compute similarity here as well\n",
    "    logits=tf.nn.xw_plus_b(outputs_concat,w,b)\n",
    "    print((logits).shape)\n",
    "    \n",
    "    #Compute one hot encodings\n",
    "    label_batch=tf.concat(train_labels,0)    \n",
    "    label_batch=27*label_batch[:,0]+label_batch[:,1]\n",
    "    print(label_batch.shape)\n",
    "    sparse_labels = tf.reshape(label_batch, [-1, 1])\n",
    "    derived_size = tf.shape(label_batch)[0]\n",
    "    indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n",
    "    print(indices.shape,'indices.shape')\n",
    "    concated = tf.concat([indices, sparse_labels],1)\n",
    "    outshape = tf.stack([derived_size, vocabulary_size])\n",
    "    labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  print(train_prediction.shape)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  #valid_dataset=np.array([i for i in range(27)])\n",
    "  #valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    " \n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  embedded_sample=tf.nn.embedding_lookup(embeddings,sample_input)\n",
    "  sample_output, sample_state = lstm_cell(embedded_sample, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        \n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    #similarity = tf.matmul(sample_prediction, tf.transpose(normalized_embeddings))\n",
    "    print(sample_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.593503 learning rate: 10.000000\n",
      "================================================================================\n",
      "ggzpkfatftrjeyvinkihna bibnlpxjixlzsogxflmyenxtlvvlr\n",
      "ykpxvcldktbegoudkmsqvwprgtnbosfsyjqxebeo iuqbivfjcgg\n",
      "ndvwwgcjvwydolkjggycfbsrlzdlnuvaqccslbfegzrloivpexpv\n",
      "btvgkf  duivafnnuyzwlkmj lkochmlcpidvkdfxvtmmcfokmvg\n",
      "jslwjbrovqwqbznaxyoyqpzshknsubmbffjvmz goygdikxkldqy\n",
      "================================================================================\n",
      "Average loss at step 500: 2.399468 learning rate: 10.000000\n",
      "Average loss at step 1000: 1.741558 learning rate: 10.000000\n",
      "================================================================================\n",
      "ihqe nounged pricles to return their list of thet mu\n",
      "hxnt directx cervan d while beywire one nine nine fo\n",
      "mwctia sgent filtinuand most euported controse s sec\n",
      "sqibential comporks tirent stancest ting too reign r\n",
      "vlor the mide and by a surrom days america prish emp\n",
      "================================================================================\n",
      "Average loss at step 1500: 1.658931 learning rate: 10.000000\n",
      "Average loss at step 2000: 1.593923 learning rate: 10.000000\n",
      "================================================================================\n",
      "zrrench his most a game one nine attemption to he wi\n",
      "pppone one one six five nine culture loss ofyiudency\n",
      "zucatios extraculating and exishines one tramen wher\n",
      "oss to the smocup siltimes others went the haysey ar\n",
      "bngaclasations a planz que editcully partional imple\n",
      "================================================================================\n",
      "Average loss at step 2500: 1.562919 learning rate: 10.000000\n",
      "Average loss at step 3000: 1.525576 learning rate: 10.000000\n",
      "================================================================================\n",
      "xp nines that an essibl of musical decrops lrhester \n",
      "wzral comparows was camble states war controductly e\n",
      "dwwhones notems an appeared to matheses council of u\n",
      "udden he language war two veragure of the comic it c\n",
      "pmman crainitable is a fir her inferuged incholer an\n",
      "================================================================================\n",
      "Average loss at step 3500: 1.550643 learning rate: 10.000000\n",
      "Average loss at step 4000: 1.535663 learning rate: 10.000000\n",
      "================================================================================\n",
      "qs planer of eart alged schoreina a bloers in the mo\n",
      "bhh slited for the king this the henry force the phi\n",
      "rlliday the ure the ruth mankeering of esc the folit\n",
      "gcanter physic detales his onice d ventions such a t\n",
      "bggraf and greatess don head the uzerstisivin begen \n",
      "================================================================================\n",
      "Average loss at step 4500: 1.516052 learning rate: 10.000000\n",
      "Average loss at step 5000: 1.527192 learning rate: 10.000000\n",
      "================================================================================\n",
      "cmmy infects ham air of haviline faiton six to these\n",
      "ibble in pett of he this for resad and universian wa\n",
      "lddsaring to skruble and gann giaditional robert lib\n",
      "nuus because of rio chalon solved as the recordin ma\n",
      "mrr from dissore cooved on birth from mult value of \n",
      "================================================================================\n",
      "Average loss at step 5500: 1.503751 learning rate: 10.000000\n",
      "Average loss at step 6000: 1.515223 learning rate: 10.000000\n",
      "================================================================================\n",
      "saaration these the noceed a does war discotability \n",
      "urrons american supposed and army runs oceaniforks a\n",
      "rffaid public govantial excecable outo wrolamations \n",
      "mg at country and when fssus track curry diefa lossi\n",
      "hnn armile ist of mecardian standantes tyler takeral\n",
      "================================================================================\n",
      "Average loss at step 6500: 1.525350 learning rate: 10.000000\n",
      "Average loss at step 7000: 1.516476 learning rate: 10.000000\n",
      "================================================================================\n",
      " zzero three the requirion n sevent this the holline\n",
      "q  study floy political cirth waters from the intern\n",
      "abble of miklable point bthted countress is a venths\n",
      "bkvular for trafferes in mars convaligm and sim webu\n",
      "waa in threse defeat four zero eight georginic can r\n",
      "================================================================================\n",
      "Average loss at step 7500: 1.527310 learning rate: 10.000000\n",
      "Average loss at step 8000: 1.499840 learning rate: 10.000000\n",
      "================================================================================\n",
      "peer gregid one in the worst of recorded the informa\n",
      "kyyop was he but would this styles acropistory a one\n",
      "uhhistering nine evolines an and bourn seen zero eit\n",
      "raator biogress and this and warfare p foliciture wh\n",
      "cdd a russion afroged it neat wavemberst persumen th\n",
      "================================================================================\n",
      "Average loss at step 8500: 1.506366 learning rate: 10.000000\n",
      "Average loss at step 9000: 1.502462 learning rate: 10.000000\n",
      "================================================================================\n",
      "qvscale writer occupations of listel common places w\n",
      "bqoeful d one for one article and normated by the so\n",
      "jxr is government ramma not for elving shiftes offic\n",
      "lppe anther acquird a codity expecing two to hers fo\n",
      "faaffrastered as localsm codo rather it has men used\n",
      "================================================================================\n",
      "Average loss at step 9500: 1.491510 learning rate: 10.000000\n",
      "Average loss at step 10000: 1.487080 learning rate: 1.000000\n",
      "================================================================================\n",
      "gaant descrimi prutal le and the voisptable whera th\n",
      "jfr of the kophestria sologially in fifth but the mo\n",
      "reewhat in list d fifteen the air one nine th wel ca\n",
      "egge and handle zero of they waltically the royck ne\n",
      "qkale embasuated companyted to the over primate huma\n",
      "================================================================================\n",
      "Average loss at step 10500: 1.460810 learning rate: 1.000000\n",
      "Average loss at step 11000: 1.447598 learning rate: 1.000000\n",
      "================================================================================\n",
      "sjjage and pagimes lenic many and circess and allunt\n",
      "gwwon only becomist undergated from fancily rail for\n",
      "mrr againg the proveries from these islamie more rec\n",
      "ybboush old one nine nine eight five two two zero d \n",
      "naabance twenties haciffaces in a plures yevilism wh\n",
      "================================================================================\n",
      "Average loss at step 11500: 1.454794 learning rate: 1.000000\n",
      "Average loss at step 12000: 1.420679 learning rate: 1.000000\n",
      "================================================================================\n",
      "r  to weapoes by perfect si life that least of claud\n",
      "ioons are their the prehourrap year of time area vol\n",
      "zkaer controtation of name sj localrly noble of the \n",
      " mmoved connections who wrised anyty two zero zero z\n",
      "skky befield the equating the compician life awards \n",
      "================================================================================\n",
      "Average loss at step 12500: 1.443823 learning rate: 1.000000\n",
      "Average loss at step 13000: 1.449472 learning rate: 1.000000\n",
      "================================================================================\n",
      "otther measur biolation to celevatural the world war\n",
      "jpsox seves male two zero hts to developloni hat sof\n",
      "ftteldent process of video variaters be obtco in bui\n",
      "faa has ganing release santer time brownk q launitin\n",
      "ewworch six four the unitagely which data very eleme\n",
      "================================================================================\n",
      "Average loss at step 13500: 1.447044 learning rate: 1.000000\n",
      "Average loss at step 14000: 1.435630 learning rate: 1.000000\n",
      "================================================================================\n",
      "woould had inland declare hoton meaning what john s \n",
      "pnnian amilitary increases two zero zero zero zero z\n",
      "oee interposed and s next to himsterings in the robe\n",
      "ycope in to two internantland this with a june embra\n",
      "mxm tewords withuare actoris the treated by all are \n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-69e03e58b3f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m       \u001b[1;31m#print((feed_dict[train_data[i]]).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_lab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 50001\n",
    "summary_frequency = 500\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "      #print((feed_dict[train_data[i]]).shape)\n",
    "    \n",
    "    _, l, predictions, lr,train_lab = session.run([optimizer, loss, train_prediction, learning_rate,train_labels], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      \n",
    "      #print(train_lab)\n",
    "      #print(labels.shape[0])\n",
    "      #print('Minibatch perplexity: ',np.exp(logprob(predictions, labels)))\n",
    "      \n",
    "      if step % (summary_frequency * 2) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "          feed[0,] =np.random.randint(0,729)\n",
    "          sentence=id2char(np.floor_divide(feed[0],27))\n",
    "          sentence+=id2char(feed[0]-27*np.floor_divide(feed[0],27)) \n",
    "          reset_sample_state.run()\n",
    "          for _ in range(50):\n",
    "            prediction=sample_prediction.eval({sample_input:feed})\n",
    "            k = sample(prediction)\n",
    "            k=characters2(k)\n",
    "            #print(k)\n",
    "            feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "            feed[0,] = 27*char2id(k[0])+char2id(k[1])\n",
    "            sentence += k[0]\n",
    "            #sentence+=k[1]\n",
    "            #feed = np.zeros(shape=(1,), dtype=np.int32)\n",
    "            #feed[0,] = np.argmax(prediction)\n",
    "            #print(feed.shape)\n",
    "            #sentence += id2char(feed[0,])\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      #for _ in range(valid_size):\n",
    "       # b = valid_batches.next()\n",
    "        #predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        #valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      #print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
