{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> gathered id2line dictionary.\n",
      "\n",
      "[['L447', 'L448'], ['L490', 'L491'], ['L716', 'L717', 'L718', 'L719', 'L720', 'L721'], ['L750', 'L751', 'L752', 'L753', 'L754', 'L755']]\n",
      ">> gathered conversations.\n",
      "\n",
      "\n",
      ">> Filter lines\n",
      "\n",
      ">> 2nd layer of filtering\n",
      "28% filtered from original data\n",
      "q : [you hate me dont you]; a : [i dont really think you warrant that strong an emotion]\n",
      "q : [then say youll spend dollar night at the track with me]; a : [and why would i do that]\n",
      "q : [come on  the ponies the flat beer you with money in your eyes me with my hand on your ass]; a : [you  covered in my vomit]\n",
      "q : [are you following me]; a : [i was in the laundromat i saw your car thought id say hi]\n",
      "\n",
      ">> Segment lines into words\n",
      "\n",
      ":: Sample from segmented list of words\n",
      "q : [['you', 'hate', 'me', 'dont', 'you']]; a : [['i', 'dont', 'really', 'think', 'you', 'warrant', 'that', 'strong', 'an', 'emotion']]\n",
      "q : [['then', 'say', 'youll', 'spend', 'dollar', 'night', 'at', 'the', 'track', 'with', 'me']]; a : [['and', 'why', 'would', 'i', 'do', 'that']]\n",
      "q : [['come', 'on', 'the', 'ponies', 'the', 'flat', 'beer', 'you', 'with', 'money', 'in', 'your', 'eyes', 'me', 'with', 'my', 'hand', 'on', 'your', 'ass']]; a : [['you', 'covered', 'in', 'my', 'vomit']]\n",
      "q : [['are', 'you', 'following', 'me']]; a : [['i', 'was', 'in', 'the', 'laundromat', 'i', 'saw', 'your', 'car', 'thought', 'id', 'say', 'hi']]\n",
      "\n",
      " >> Index words\n",
      "\n",
      " >> Filter Unknowns\n",
      "2% filtered from original data\n",
      "\n",
      " Final dataset len : 96467\n",
      "\n",
      " >> Zero Padding\n",
      "\n",
      " >> Save numpy arrays to disk\n",
      "% unknown : 4.2709945477651665\n",
      "Dataset count : 96467\n"
     ]
    }
   ],
   "source": [
    "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz ' # space is included in whitelist\n",
    "EN_BLACKLIST = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\''\n",
    "\n",
    "limit = {\n",
    "        'maxq' : 25,\n",
    "        'minq' : 2,\n",
    "        'maxa' : 25,\n",
    "        'mina' : 2\n",
    "        }\n",
    "\n",
    "UNK = 'unk'\n",
    "VOCAB_SIZE = 8000\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "''' \n",
    "    1. Read from 'movie-lines.txt'\n",
    "    2. Create a dictionary with ( key = line_id, value = text )\n",
    "'''\n",
    "def get_id2line():\n",
    "    lines=open('raw_data/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "    id2line = {}\n",
    "    for line in lines:\n",
    "        _line = line.split(' +++$+++ ')\n",
    "        if len(_line) == 5:\n",
    "            id2line[_line[0]] = _line[4]\n",
    "    return id2line\n",
    "\n",
    "'''\n",
    "    1. Read from 'movie_conversations.txt'\n",
    "    2. Create a list of [list of line_id's]\n",
    "'''\n",
    "def get_conversations():\n",
    "    conv_lines = open('raw_data/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "    convs = [ ]\n",
    "    for line in conv_lines[:-1]:\n",
    "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "        convs.append(_line.split(','))\n",
    "    return convs\n",
    "\n",
    "'''\n",
    "    1. Get each conversation\n",
    "    2. Get each line from conversation\n",
    "    3. Save each conversation to file\n",
    "'''\n",
    "def extract_conversations(convs,id2line,path=''):\n",
    "    idx = 0\n",
    "    for conv in convs:\n",
    "        f_conv = open(path + str(idx)+'.txt', 'w')\n",
    "        for line_id in conv:\n",
    "            f_conv.write(id2line[line_id])\n",
    "            f_conv.write('\\n')\n",
    "        f_conv.close()\n",
    "        idx += 1\n",
    "\n",
    "'''\n",
    "    Get lists of all conversations as Questions and Answers\n",
    "    1. [questions]\n",
    "    2. [answers]\n",
    "'''\n",
    "def gather_dataset(convs, id2line):\n",
    "    questions = []; answers = []\n",
    "\n",
    "    for conv in convs:\n",
    "        if len(conv) %2 != 0:\n",
    "            conv = conv[:-1]\n",
    "        for i in range(len(conv)):\n",
    "            if i%2 == 0:\n",
    "                questions.append(id2line[conv[i]])\n",
    "            else:\n",
    "                answers.append(id2line[conv[i]])\n",
    "\n",
    "    return questions, answers\n",
    "\n",
    "\n",
    "'''\n",
    "    We need 4 files\n",
    "    1. train.enc : Encoder input for training\n",
    "    2. train.dec : Decoder input for training\n",
    "    3. test.enc  : Encoder input for testing\n",
    "    4. test.dec  : Decoder input for testing\n",
    "'''\n",
    "def prepare_seq2seq_files(questions, answers, path='',TESTSET_SIZE = 30000):\n",
    "    \n",
    "    # open files\n",
    "    train_enc = open(path + 'train.enc','w')\n",
    "    train_dec = open(path + 'train.dec','w')\n",
    "    test_enc  = open(path + 'test.enc', 'w')\n",
    "    test_dec  = open(path + 'test.dec', 'w')\n",
    "\n",
    "    # choose 30,000 (TESTSET_SIZE) items to put into testset\n",
    "    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        if i in test_ids:\n",
    "            test_enc.write(questions[i]+'\\n')\n",
    "            test_dec.write(answers[i]+ '\\n' )\n",
    "        else:\n",
    "            train_enc.write(questions[i]+'\\n')\n",
    "            train_dec.write(answers[i]+ '\\n' )\n",
    "        if i%10000 == 0:\n",
    "            print('\\n>> written {} lines'.format(i))\n",
    "\n",
    "    # close files\n",
    "    train_enc.close()\n",
    "    train_dec.close()\n",
    "    test_enc.close()\n",
    "    test_dec.close()\n",
    "            \n",
    "\n",
    "\n",
    "'''\n",
    " remove anything that isn't in the vocabulary\n",
    "    return str(pure en)\n",
    "\n",
    "'''\n",
    "def filter_line(line, whitelist):\n",
    "    return ''.join([ ch for ch in line if ch in whitelist ])\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    " filter too long and too short sequences\n",
    "    return tuple( filtered_ta, filtered_en )\n",
    "\n",
    "'''\n",
    "def filter_data(qseq, aseq):\n",
    "    filtered_q, filtered_a = [], []\n",
    "    raw_data_len = len(qseq)\n",
    "\n",
    "    assert len(qseq) == len(aseq)\n",
    "\n",
    "    for i in range(raw_data_len):\n",
    "        qlen, alen = len(qseq[i].split(' ')), len(aseq[i].split(' '))\n",
    "        if qlen >= limit['minq'] and qlen <= limit['maxq']:\n",
    "            if alen >= limit['mina'] and alen <= limit['maxa']:\n",
    "                filtered_q.append(qseq[i])\n",
    "                filtered_a.append(aseq[i])\n",
    "\n",
    "    # print the fraction of the original data, filtered\n",
    "    filt_data_len = len(filtered_q)\n",
    "    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n",
    "    print(str(filtered) + '% filtered from original data')\n",
    "\n",
    "    return filtered_q, filtered_a\n",
    "\n",
    "\n",
    "'''\n",
    " read list of words, create index to word,\n",
    "  word to index dictionaries\n",
    "    return tuple( vocab->(word, count), idx2w, w2idx )\n",
    "\n",
    "'''\n",
    "def index_(tokenized_sentences, vocab_size):\n",
    "    # get frequency distribution\n",
    "    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    # get vocabulary of 'vocab_size' most used words\n",
    "    vocab = freq_dist.most_common(vocab_size)\n",
    "    # index2word\n",
    "    index2word = ['_'] + [UNK] + [ x[0] for x in vocab ]\n",
    "    # word2index\n",
    "    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n",
    "    return index2word, word2index, freq_dist\n",
    "\n",
    "'''\n",
    " filter based on number of unknowns (words not in vocabulary)\n",
    "  filter out the worst sentences\n",
    "\n",
    "'''\n",
    "def filter_unk(qtokenized, atokenized, w2idx):\n",
    "    data_len = len(qtokenized)\n",
    "\n",
    "    filtered_q, filtered_a = [], []\n",
    "\n",
    "    for qline, aline in zip(qtokenized, atokenized):\n",
    "        unk_count_q = len([ w for w in qline if w not in w2idx ])\n",
    "        unk_count_a = len([ w for w in aline if w not in w2idx ])\n",
    "        if unk_count_a <= 2:\n",
    "            if unk_count_q > 0:\n",
    "                if unk_count_q/len(qline) > 0.2:\n",
    "                    pass\n",
    "            filtered_q.append(qline)\n",
    "            filtered_a.append(aline)\n",
    "\n",
    "    # print the fraction of the original data, filtered\n",
    "    filt_data_len = len(filtered_q)\n",
    "    filtered = int((data_len - filt_data_len)*100/data_len)\n",
    "    print(str(filtered) + '% filtered from original data')\n",
    "\n",
    "    return filtered_q, filtered_a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    " create the final dataset : \n",
    "  - convert list of items to arrays of indices\n",
    "  - add zero padding\n",
    "      return ( [array_en([indices]), array_ta([indices]) )\n",
    " \n",
    "'''\n",
    "def zero_pad(qtokenized, atokenized, w2idx):\n",
    "    # num of rows\n",
    "    data_len = len(qtokenized)\n",
    "\n",
    "    # numpy arrays to store indices\n",
    "    idx_q = np.zeros([data_len, limit['maxq']], dtype=np.int32) \n",
    "    idx_a = np.zeros([data_len, limit['maxa']], dtype=np.int32)\n",
    "\n",
    "    for i in range(data_len):\n",
    "        q_indices = pad_seq(qtokenized[i], w2idx, limit['maxq'])\n",
    "        a_indices = pad_seq(atokenized[i], w2idx, limit['maxa'])\n",
    "\n",
    "        #print(len(idx_q[i]), len(q_indices))\n",
    "        #print(len(idx_a[i]), len(a_indices))\n",
    "        idx_q[i] = np.array(q_indices)\n",
    "        idx_a[i] = np.array(a_indices)\n",
    "\n",
    "    return idx_q, idx_a\n",
    "\n",
    "\n",
    "'''\n",
    " replace words with indices in a sequence\n",
    "  replace with unknown if word not in lookup\n",
    "    return [list of indices]\n",
    "\n",
    "'''\n",
    "def pad_seq(seq, lookup, maxlen):\n",
    "    indices = []\n",
    "    for word in seq:\n",
    "        if word in lookup:\n",
    "            indices.append(lookup[word])\n",
    "        else:\n",
    "            indices.append(lookup[UNK])\n",
    "    return indices + [0]*(maxlen - len(seq))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_data():\n",
    "\n",
    "    id2line = get_id2line()\n",
    "    print('>> gathered id2line dictionary.\\n')\n",
    "    convs = get_conversations()\n",
    "    print(convs[121:125])\n",
    "    print('>> gathered conversations.\\n')\n",
    "    questions, answers = gather_dataset(convs,id2line)\n",
    "\n",
    "    # change to lower case (just for en)\n",
    "    questions = [ line.lower() for line in questions ]\n",
    "    answers = [ line.lower() for line in answers ]\n",
    "\n",
    "    # filter out unnecessary characters\n",
    "    print('\\n>> Filter lines')\n",
    "    questions = [ filter_line(line, EN_WHITELIST) for line in questions ]\n",
    "    answers = [ filter_line(line, EN_WHITELIST) for line in answers ]\n",
    "\n",
    "    # filter out too long or too short sequences\n",
    "    print('\\n>> 2nd layer of filtering')\n",
    "    qlines, alines = filter_data(questions, answers)\n",
    "\n",
    "    for q,a in zip(qlines[141:145], alines[141:145]):\n",
    "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
    "\n",
    "    # convert list of [lines of text] into list of [list of words ]\n",
    "    print('\\n>> Segment lines into words')\n",
    "    qtokenized = [ [w.strip() for w in wordlist.split(' ') if w] for wordlist in qlines ]\n",
    "    atokenized = [ [w.strip() for w in wordlist.split(' ') if w] for wordlist in alines ]\n",
    "    print('\\n:: Sample from segmented list of words')\n",
    "\n",
    "    for q,a in zip(qtokenized[141:145], atokenized[141:145]):\n",
    "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
    "\n",
    "    # indexing -> idx2w, w2idx \n",
    "    print('\\n >> Index words')\n",
    "    idx2w, w2idx, freq_dist = index_( qtokenized + atokenized, vocab_size=VOCAB_SIZE)\n",
    "    \n",
    "    # filter out sentences with too many unknowns\n",
    "    print('\\n >> Filter Unknowns')\n",
    "    qtokenized, atokenized = filter_unk(qtokenized, atokenized, w2idx)\n",
    "    print('\\n Final dataset len : ' + str(len(qtokenized)))\n",
    "\n",
    "\n",
    "    print('\\n >> Zero Padding')\n",
    "    idx_q, idx_a = zero_pad(qtokenized, atokenized, w2idx)\n",
    "\n",
    "    print('\\n >> Save numpy arrays to disk')\n",
    "    # save them\n",
    "    np.save('idx_q.npy', idx_q)\n",
    "    np.save('idx_a.npy', idx_a)\n",
    "\n",
    "    # let us now save the necessary dictionaries\n",
    "    metadata = {\n",
    "            'w2idx' : w2idx,\n",
    "            'idx2w' : idx2w,\n",
    "            'limit' : limit,\n",
    "            'freq_dist' : freq_dist\n",
    "                }\n",
    "\n",
    "    # write to disk : data control dictionaries\n",
    "    with open('metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "    # count of unknowns\n",
    "    unk_count = (idx_q == 1).sum() + (idx_a == 1).sum()\n",
    "    # count of words\n",
    "    word_count = (idx_q > 1).sum() + (idx_a > 1).sum()\n",
    "\n",
    "    print('% unknown : {0}'.format(100 * (unk_count/word_count)))\n",
    "    print('Dataset count : ' + str(idx_q.shape[0]))\n",
    "\n",
    "\n",
    "    #print '>> gathered questions and answers.\\n'\n",
    "    #prepare_seq2seq_files(questions,answers)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_data()\n",
    "\n",
    "\n",
    "def load_data(PATH=''):\n",
    "    # read data control dictionaries\n",
    "    with open(PATH + 'metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    # read numpy arrays\n",
    "    idx_q = np.load(PATH + 'idx_q.npy')\n",
    "    idx_a = np.load(PATH + 'idx_a.npy')\n",
    "    return metadata, idx_q, idx_a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
