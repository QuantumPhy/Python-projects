{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --min_word_frequency: conflicting option string: --min_word_frequency",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c8cf59a75845>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'min_word_frequency'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Minimum frequency of words in the vocabulary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"max_sentence_len\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m160\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Maximum Sentence Length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py\u001b[0m in \u001b[0;36mDEFINE_integer\u001b[1;34m(flag_name, default_value, docstring)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mdocstring\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mhelpful\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0mexplaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m   \"\"\"\n\u001b[1;32m---> 91\u001b[1;33m   \u001b[0m_define_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflag_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py\u001b[0m in \u001b[0;36m_define_helper\u001b[1;34m(flag_name, default_value, docstring, flagtype)\u001b[0m\n\u001b[0;32m     63\u001b[0m                               \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                               \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                               type=flagtype)\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36madd_argument\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1342\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"length of metavar tuple does not match nargs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1705\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1707\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1708\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1709\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1547\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1548\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1549\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m         \u001b[1;31m# resolve any conflicts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[1;31m# add to actions list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36m_check_conflict\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1495\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1497\u001b[1;33m             \u001b[0mconflict_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ricsi\\Anaconda3\\envs\\tensorflow\\lib\\argparse.py\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[1;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[0;32m   1504\u001b[0m                                      \u001b[1;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1505\u001b[0m                                      in conflicting_actions])\n\u001b[1;32m-> 1506\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1508\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument --min_word_frequency: conflicting option string: --min_word_frequency"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import array\n",
    "\n",
    "tf.flags.DEFINE_integer(\"min_word_frequency\", 5, \"Minimum frequency of words in the vocabulary\")\n",
    "\n",
    "tf.flags.DEFINE_integer(\"max_sentence_len\", 160, \"Maximum Sentence Length\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "  \"input_dir\", os.path.abspath(\"./data\"),\n",
    "  \"Input directory containing original CSV data files (default = './data')\")\n",
    "\n",
    "tf.flags.DEFINE_string(\n",
    "  \"output_dir\", os.path.abspath(\"./data\"),\n",
    "  \"Output directory for TFrEcord files (default = './data')\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "TRAIN_PATH = os.path.join(FLAGS.input_dir, \"train.csv\")\n",
    "VALIDATION_PATH = os.path.join(FLAGS.input_dir, \"valid.csv\")\n",
    "TEST_PATH = os.path.join(FLAGS.input_dir, \"test.csv\")\n",
    "\n",
    "def tokenizer_fn(iterator):\n",
    "  return (x.split(\" \") for x in iterator)\n",
    "\n",
    "def create_csv_iter(filename):\n",
    "  \"\"\"\n",
    "  Returns an iterator over a CSV file. Skips the header.\n",
    "  \"\"\"\n",
    "  with open(filename) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # Skip the header\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "      yield row\n",
    "\n",
    "\n",
    "def create_vocab(input_iter, min_frequency):\n",
    "  \"\"\"\n",
    "  Creates and returns a VocabularyProcessor object with the vocabulary\n",
    "  for the input iterator.\n",
    "  \"\"\"\n",
    "  vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "      FLAGS.max_sentence_len,\n",
    "      min_frequency=min_frequency,\n",
    "      tokenizer_fn=tokenizer_fn)\n",
    "  vocab_processor.fit(input_iter)\n",
    "  return vocab_processor\n",
    "\n",
    "\n",
    "def transform_sentence(sequence, vocab_processor):\n",
    "  \"\"\"\n",
    "  Maps a single sentence into the integer vocabulary. Returns a python array.\n",
    "  \"\"\"\n",
    "  return next(vocab_processor.transform([sequence])).tolist()\n",
    "\n",
    "\n",
    "def create_text_sequence_feature(fl, sentence, sentence_len, vocab):\n",
    "  \"\"\"\n",
    "  Writes a sentence to FeatureList protocol buffer\n",
    "  \"\"\"\n",
    "  sentence_transformed = transform_sentence(sentence, vocab)\n",
    "  for word_id in sentence_transformed:\n",
    "    fl.feature.add().int64_list.value.extend([word_id])\n",
    "  return fl\n",
    "\n",
    "\n",
    "def create_example_train(row, vocab):\n",
    "  \"\"\"\n",
    "  Creates a training example for the Ubuntu Dialog Corpus dataset.\n",
    "  Returnsthe a tensorflow.Example Protocol Buffer object.\n",
    "  \"\"\"\n",
    "  context, utterance, label = row\n",
    "  context_transformed = transform_sentence(context, vocab)\n",
    "  utterance_transformed = transform_sentence(utterance, vocab)\n",
    "  context_len = len(next(vocab._tokenizer([context])))\n",
    "  utterance_len = len(next(vocab._tokenizer([utterance])))\n",
    "  label = int(float(label))\n",
    "\n",
    "  # New Example\n",
    "  example = tf.train.Example()\n",
    "  example.features.feature[\"context\"].int64_list.value.extend(context_transformed)\n",
    "  example.features.feature[\"utterance\"].int64_list.value.extend(utterance_transformed)\n",
    "  example.features.feature[\"context_len\"].int64_list.value.extend([context_len])\n",
    "  example.features.feature[\"utterance_len\"].int64_list.value.extend([utterance_len])\n",
    "  example.features.feature[\"label\"].int64_list.value.extend([label])\n",
    "  return example\n",
    "\n",
    "\n",
    "def create_example_test(row, vocab):\n",
    "  \"\"\"\n",
    "  Creates a test/validation example for the Ubuntu Dialog Corpus dataset.\n",
    "  Returnsthe a tensorflow.Example Protocol Buffer object.\n",
    "  \"\"\"\n",
    "  context, utterance = row[:2]\n",
    "  distractors = row[2:]\n",
    "  context_len = len(next(vocab._tokenizer([context])))\n",
    "  utterance_len = len(next(vocab._tokenizer([utterance])))\n",
    "  context_transformed = transform_sentence(context, vocab)\n",
    "  utterance_transformed = transform_sentence(utterance, vocab)\n",
    "\n",
    "  # New Example\n",
    "  example = tf.train.Example()\n",
    "  example.features.feature[\"context\"].int64_list.value.extend(context_transformed)\n",
    "  example.features.feature[\"utterance\"].int64_list.value.extend(utterance_transformed)\n",
    "  example.features.feature[\"context_len\"].int64_list.value.extend([context_len])\n",
    "  example.features.feature[\"utterance_len\"].int64_list.value.extend([utterance_len])\n",
    "\n",
    "  # Distractor sequences\n",
    "  for i, distractor in enumerate(distractors):\n",
    "    dis_key = \"distractor_{}\".format(i)\n",
    "    dis_len_key = \"distractor_{}_len\".format(i)\n",
    "    # Distractor Length Feature\n",
    "    dis_len = len(next(vocab._tokenizer([distractor])))\n",
    "    example.features.feature[dis_len_key].int64_list.value.extend([dis_len])\n",
    "    # Distractor Text Feature\n",
    "    dis_transformed = transform_sentence(distractor, vocab)\n",
    "    example.features.feature[dis_key].int64_list.value.extend(dis_transformed)\n",
    "  return example\n",
    "\n",
    "\n",
    "def create_tfrecords_file(input_filename, output_filename, example_fn):\n",
    "  \"\"\"\n",
    "  Creates a TFRecords file for the given input data and\n",
    "  example transofmration function\n",
    "  \"\"\"\n",
    "  writer = tf.python_io.TFRecordWriter(output_filename)\n",
    "  print(\"Creating TFRecords file at {}...\".format(output_filename))\n",
    "  for i, row in enumerate(create_csv_iter(input_filename)):\n",
    "    x = example_fn(row)\n",
    "    writer.write(x.SerializeToString())\n",
    "  writer.close()\n",
    "  print(\"Wrote to {}\".format(output_filename))\n",
    "\n",
    "\n",
    "def write_vocabulary(vocab_processor, outfile):\n",
    "  \"\"\"\n",
    "  Writes the vocabulary to a file, one word per line.\n",
    "  \"\"\"\n",
    "  vocab_size = len(vocab_processor.vocabulary_)\n",
    "  with open(outfile, \"w\") as vocabfile:\n",
    "    for id in range(vocab_size):\n",
    "      word =  vocab_processor.vocabulary_._reverse_mapping[id]\n",
    "      vocabfile.write(word + \"\\n\")\n",
    "  print(\"Saved vocabulary to {}\".format(outfile))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  print(\"Creating vocabulary...\")\n",
    "  input_iter = create_csv_iter(TRAIN_PATH)\n",
    "  input_iter = (x[0] + \" \" + x[1] for x in input_iter)\n",
    "  vocab = create_vocab(input_iter, min_frequency=FLAGS.min_word_frequency)\n",
    "  print(\"Total vocabulary size: {}\".format(len(vocab.vocabulary_)))\n",
    "\n",
    "  # Create vocabulary.txt file\n",
    "  write_vocabulary(\n",
    "    vocab, os.path.join(FLAGS.output_dir, \"vocabulary.txt\"))\n",
    "\n",
    "  # Save vocab processor\n",
    "  vocab.save(os.path.join(FLAGS.output_dir, \"vocab_processor.bin\"))\n",
    "\n",
    "  # Create validation.tfrecords\n",
    "  create_tfrecords_file(\n",
    "      input_filename=VALIDATION_PATH,\n",
    "      output_filename=os.path.join(FLAGS.output_dir, \"validation.tfrecords\"),\n",
    "      example_fn=functools.partial(create_example_test, vocab=vocab))\n",
    "\n",
    "  # Create test.tfrecords\n",
    "  create_tfrecords_file(\n",
    "      input_filename=TEST_PATH,\n",
    "      output_filename=os.path.join(FLAGS.output_dir, \"test.tfrecords\"),\n",
    "      example_fn=functools.partial(create_example_test, vocab=vocab))\n",
    "\n",
    "  # Create train.tfrecords\n",
    "  create_tfrecords_file(\n",
    "      input_filename=TRAIN_PATH,\n",
    "      output_filename=os.path.join(FLAGS.output_dir, \"train.tfrecords\"),\n",
    "      example_fn=functools.partial(create_example_train, vocab=vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
